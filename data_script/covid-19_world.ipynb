{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Source COVID World updated everyday at 2:00 AM : </b> <br> \n",
    "https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_time_series <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to MongoDB\n"
     ]
    }
   ],
   "source": [
    "%run __init__.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe WORLD_CONFIRMED successfully save in database covid-19-dev in MongoDB. Time: --- 0.0682826042175293 secnds ---\n",
      "Dataframe WORLD_DEATHS successfully save in database covid-19-dev in MongoDB. Time: --- 0.06247282028198242 secnds ---\n",
      "Dataframe WORLD_RECOVERED successfully save in database covid-19-dev in MongoDB. Time: --- 0.07368993759155273 secnds ---\n",
      "Dataframe US_CONFIRMED successfully save in database covid-19-dev in MongoDB. Time: --- 0.062105417251586914 secnds ---\n",
      "Dataframe US_DEATHS successfully save in database covid-19-dev in MongoDB. Time: --- 0.06197047233581543 secnds ---\n",
      "Script execution completed at 13/04/2020 22:42:15. Time: --- 27.603705406188965 secnds ---\n"
     ]
    }
   ],
   "source": [
    "#--- STEP1 ---: Get data source & cleaning\n",
    "start_time = time.time()\n",
    "\n",
    "#-- GITHUB : TIMESERIES (CSSEGISandData/COVID-19/csse_covid_19_time_series)\n",
    "#-> WORLD_CONFIRMED\n",
    "urlConfirmed = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv'\n",
    "cConfirmed=check_url(urlConfirmed)\n",
    "if use_mongo:\n",
    "    bob.mongo.save_df(cConfirmed,'WORLD_CONFIRMED',db_src,True)\n",
    "\n",
    "#-> WORLD_DEATHS\n",
    "urlDeath = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv'\n",
    "cDeath=check_url(urlDeath)\n",
    "if use_mongo:\n",
    "    bob.mongo.save_df(cDeath,'WORLD_DEATHS',db_src,True)\n",
    "\n",
    "#-> WORLD_RECOVERED\n",
    "urlRecovery = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_recovered_global.csv'\n",
    "cRecovery=check_url(urlRecovery)\n",
    "if use_mongo:\n",
    "    bob.mongo.save_df(cRecovery,'WORLD_RECOVERED',db_src,True)\n",
    "\n",
    "#-> US_CONFIRMED\n",
    "urlConfirmedUS = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv'\n",
    "cConfirmedUS=check_url(urlConfirmedUS)\n",
    "if use_mongo:\n",
    "    bob.mongo.save_df(cConfirmed,'US_CONFIRMED',db_src,True)\n",
    "cConfirmedUS.to_csv(output_folder + 'US_CONFIRMED.csv',sep=\";\") \n",
    "\n",
    "#-> US_DEATHS\n",
    "urlDeathUS = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_US.csv'\n",
    "cDeathUS=check_url(urlDeathUS)\n",
    "if use_mongo:\n",
    "    bob.mongo.save_df(cDeath,'US_DEATHS',db_src,True)\n",
    "cDeathUS.to_csv(output_folder + 'US_DEATHS.csv',sep=\";\") \n",
    "\n",
    "#-- GITHUB : DAILY REPORT (CSSEGISandData/COVID-19/csse_covid_19_time_series)\n",
    "#-> DAILY\n",
    "date_init = yesterday.strftime('%m-%d-%Y')\n",
    "date_end = yesterday.strftime('%m-%d-%Y')\n",
    "dates = pd.date_range(start=date_init,end=date_end, freq='D').strftime('%m-%d-%Y').values.tolist()\n",
    "filter_dates = pd.date_range(start=date_init,end=date_end, freq='D').strftime('%m/%d/%y').values.tolist()\n",
    "\n",
    "cDaily = pd.DataFrame()\n",
    "for d in dates:\n",
    "    urlDaily = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports/' + d + '.csv'\n",
    "    cols_to_rename = {\"Country/Region\": \"Country_Region\",\"Province/State\": \"Province_State\", \"Long_\":\"Long\"}\n",
    "    tmp_df = check_url(urlDaily).rename(index=str, columns=cols_to_rename)\n",
    "    if len(tmp_df) != 0:\n",
    "        tmp_df['Confirmed'] = tmp_df['Confirmed'].fillna(0).astype(int)\n",
    "        tmp_df['Deaths'] = tmp_df['Deaths'].fillna(0).astype(int)\n",
    "        tmp_df['Recovered'] = tmp_df['Recovered'].fillna(0).astype(int)\n",
    "        tmp_df['Date'] = pd.to_datetime(d, format='%m-%d-%Y').strftime('%m/%d/%y')\n",
    "        cDaily = cDaily.append(tmp_df)\n",
    "        \n",
    "with pd.ExcelWriter(output_folder + 'WSR_datasource_' + yesterday.strftime('%Y%m%d') + '.xlsx') as writer:  \n",
    "    cConfirmed.to_excel(writer,sheet_name =\"WORLD_CONFIRMED\")\n",
    "    cDeath.to_excel(writer,sheet_name =\"WORLD_DEATHS\")\n",
    "    cRecovery.to_excel(writer,sheet_name =\"WORLD_RECOVERED\")\n",
    "    cDaily.to_excel(writer,sheet_name =\"WORLD_DAILY\")\n",
    "    cConfirmedUS.to_excel(writer,sheet_name =\"US_CONFIRMED\")\n",
    "    cDeathUS.to_excel(writer,sheet_name =\"US_DEATHS\")\n",
    "print(\"Script execution completed at \" + datetime.now().strftime('%d/%m/%Y %H:%M:%S') + \". Time: --- %s secnds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Melt data source from timeseries to have date in rows\n",
    "def melt_cleaning(df,variable):\n",
    "    try:\n",
    "        if 'Province/State' and 'Country/Region' in df.columns:\n",
    "            #Add Country/Region to Province/State if empty\n",
    "            df = df.fillna(0)\n",
    "            df.loc[df['Province/State'] == 0,'Province/State'] = df['Country/Region']\n",
    "            #Melt and group data\n",
    "            cols_to_keep = ['Country/Region','Province/State']\n",
    "            cols_to_group = ['Country/Region','Province/State','Date']\n",
    "            df= df.melt(id_vars=cols_to_keep,value_name=variable,var_name='Date').groupby(cols_to_group, as_index=False).agg({variable:'sum'})\n",
    "            #Data format\n",
    "            df['Date'] = pd.to_datetime(df['Date'], format='%m/%d/%y').dt.strftime('%m/%d/%y')\n",
    "\n",
    "            if variable == 'Confirmed':\n",
    "                df['Deaths'] = 0\n",
    "                df['Recovered'] = 0\n",
    "            elif variable == 'Deaths':\n",
    "                df['Confirmed'] = 0\n",
    "                df['Recovered'] = 0\n",
    "            else:\n",
    "                df['Confirmed'] = 0\n",
    "                df['Deaths'] = 0\n",
    "        else:\n",
    "            df=pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        df=pd.DataFrame()\n",
    "        print(e.__doc__)\n",
    "        print(str(e))  \n",
    "    return df\n",
    "\n",
    "def cols_cleaning(df,cols_to_drop):\n",
    "    for c in cols_to_drop:\n",
    "        if c in df.columns:\n",
    "            df = df.drop(c,axis=1)\n",
    "            \n",
    "    cols_to_rename = {\"Province_State\": \"Province/State\", \"Country_Region\": \"Country/Region\"}\n",
    "    df = df.rename(index=str, columns=cols_to_rename)\n",
    "    return df       \n",
    "\n",
    "# Timeseries : World Confirmed\n",
    "cols_to_drop = ['Lat','Long']\n",
    "cConfirmed=cConfirmed[cConfirmed['Country/Region'] != 'US']\n",
    "df_cW = melt_cleaning(cols_cleaning(cConfirmed, cols_to_drop),'Confirmed')\n",
    "    \n",
    "# Timeseries : World Deaths\n",
    "cols_to_drop = ['Lat','Long']\n",
    "cDeath=cDeath[cDeath['Country/Region'] != 'US']\n",
    "df_dW = melt_cleaning(cols_cleaning(cDeath, cols_to_drop),'Deaths')\n",
    "    \n",
    "# Timeseries : World Recovered\n",
    "cols_to_drop = ['Lat','Long']\n",
    "df_rW = melt_cleaning(cols_cleaning(cRecovery, cols_to_drop),'Recovered')\n",
    "\n",
    "# Timeseries : US Confirmed\n",
    "cols_to_drop = ['UID','iso2', 'iso3','code3','FIPS','Admin2','Lat','Long_','Combined_Key']\n",
    "df_cUS = melt_cleaning(cols_cleaning(cConfirmedUS, cols_to_drop),'Confirmed')\n",
    "\n",
    "# Timeseries : US Deaths\n",
    "cols_to_drop = ['UID','iso2', 'iso3','code3','FIPS','Admin2','Lat','Long_','Combined_Key','Population']\n",
    "df_dUS = melt_cleaning(cols_cleaning(cDeathUS, cols_to_drop),'Deaths')\n",
    "\n",
    "# Manual input\n",
    "df_manual = pd.read_excel(input_folder + 'REF_WSR.xlsx', sheet_name = 'DATA_INPUT').fillna(0)\n",
    "df_manual['Date'] = pd.to_datetime(df_manual['Date'], format='%Y%m%d').dt.strftime('%m/%d/%y')\n",
    "# df_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script execution completed at 13/04/2020 12:25:31. Time: --- 0.054505348205566406 secnds ---\n",
      "Dataframe WORLD_DB_CONCAT successfully save in database covid-19-dev in MongoDB. Time: --- 0.7700128555297852 secnds ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country/Region</th>\n",
       "      <th>Province/State</th>\n",
       "      <th>Date</th>\n",
       "      <th>Confirmed</th>\n",
       "      <th>Deaths</th>\n",
       "      <th>Recovered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>01/22/20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>01/23/20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>01/24/20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>01/25/20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>01/26/20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26481</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>04/08/20</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26482</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>04/09/20</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26483</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>04/10/20</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26484</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>04/11/20</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26485</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>04/12/20</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26486 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Country/Region Province/State      Date  Confirmed  Deaths  Recovered\n",
       "0        Afghanistan    Afghanistan  01/22/20          0       0        0.0\n",
       "1        Afghanistan    Afghanistan  01/23/20          0       0        0.0\n",
       "2        Afghanistan    Afghanistan  01/24/20          0       0        0.0\n",
       "3        Afghanistan    Afghanistan  01/25/20          0       0        0.0\n",
       "4        Afghanistan    Afghanistan  01/26/20          0       0        0.0\n",
       "...              ...            ...       ...        ...     ...        ...\n",
       "26481       Zimbabwe       Zimbabwe  04/08/20         11       3        0.0\n",
       "26482       Zimbabwe       Zimbabwe  04/09/20         11       3        0.0\n",
       "26483       Zimbabwe       Zimbabwe  04/10/20         13       3        0.0\n",
       "26484       Zimbabwe       Zimbabwe  04/11/20         14       3        0.0\n",
       "26485       Zimbabwe       Zimbabwe  04/12/20         14       3        0.0\n",
       "\n",
       "[26486 rows x 6 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#--- STEP2 ---: Create DB with data source (output = DB_CONCAT)\n",
    "start_time = time.time()\n",
    "def step2(df1,df2,df3,df4,df5,df6):\n",
    "    #-- Merge timeseries\n",
    "    cols_to_group = ['Country/Region','Province/State','Date']\n",
    "    df = pd.concat([df1,df2,df3,df4,df5,df6], axis=0).groupby(cols_to_group, as_index=False).agg({'Confirmed':'sum','Deaths':'sum','Recovered':'sum'})\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "db_concat = step2(df_cW,df_dW,df_rW,df_cUS,df_dUS,df_manual)\n",
    "print(\"Script execution completed at \" + datetime.now().strftime('%d/%m/%Y %H:%M:%S') + \". Time: --- %s secnds ---\" % (time.time() - start_time))\n",
    "db_concat.to_csv(output_folder + 'WORLD_DB_CONCAT.csv',sep=\";\")\n",
    "# bob.mongo.save_df(db_concat,'WORLD_DB_CONCAT',db_src,True)\n",
    "# db_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script execution completed at 13/04/2020 12:25:32. Time: --- 0.2988009452819824 secnds ---\n",
      "Dataframe WORLD_DB_CONSO successfully save in database covid-19-dev in MongoDB. Time: --- 30.186661958694458 secnds ---\n"
     ]
    }
   ],
   "source": [
    "#--- STEP3 ---: Consolidate and enrich data (output = DB_CONSO)\n",
    "start_time = time.time()\n",
    "#-> Referentials\n",
    "ref_continent = pd.read_excel(input_folder + 'REF_WSR.xlsx', sheet_name = 'REF_CONTINENT')\n",
    "def step3(df,ref):\n",
    "    df.columns = df.columns.str.upper()\n",
    "    cols_to_rename = {\"COUNTRY/REGION\": \"ENTITY_GROUPS\", \"PROVINCE/STATE\": \"ENTITY\"}\n",
    "    df = df.rename(index=str, columns=cols_to_rename)\n",
    "    \n",
    "    cols_to_rename = {\"ENTITY_GROUPS\": \"ENTITY\", \"CONTINENT\": \"ENTITY_GROUPS\"}\n",
    "    df_con = pd.merge(df.drop(['ENTITY'],axis=1),ref.drop(['WORLDMAP'],axis=1), left_on=['ENTITY_GROUPS'], right_on=['COUNTRY_REGION'], how='left').rename(index=str, columns=cols_to_rename).drop(['COUNTRY_REGION'],axis=1).fillna(0)\n",
    "    df_con.loc[df_con['ENTITY_GROUPS'] == 0, 'ENTITY_GROUPS'] = 'To be affected'\n",
    "    \n",
    "    cols_to_rename = {\"ENTITY_GROUPS\": \"ENTITY\"}\n",
    "    df_ww = df_con.copy().drop(['ENTITY'],axis=1).rename(index=str, columns=cols_to_rename)\n",
    "    df_ww['ENTITY_GROUPS'] = 'WORLDWIDE'\n",
    "    \n",
    "    cols_to_rename = {\"ENTITY_GROUPS\": \"ENTITY\"}\n",
    "    df_tt = df_ww.copy().drop(['ENTITY'],axis=1).rename(index=str, columns=cols_to_rename)\n",
    "    df_tt['ENTITY_GROUPS'] = 'WORLDWIDE'\n",
    "\n",
    "    df = pd.concat([df,df_con,df_ww,df_tt], axis=0)\n",
    "    df = df.groupby(['ENTITY_GROUPS','ENTITY','DATE'], as_index=False).agg({'CONFIRMED': 'sum','DEATHS': 'sum', 'RECOVERED': 'sum'})\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "db_conso = step3(db_concat,ref_continent)\n",
    "print(\"Script execution completed at \" + datetime.now().strftime('%d/%m/%Y %H:%M:%S') + \". Time: --- %s secnds ---\" % (time.time() - start_time))\n",
    "db_conso.to_csv(output_folder + 'WORLD_DB_CONSO.csv',sep=\";\")\n",
    "# if use_mongo:\n",
    "#     bob.mongo.save_df(db_conso,'WORLD_DB_CONSO',db_src,True)\n",
    "# db_conso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script execution completed at 13/04/2020 12:26:07. Time: --- 4.24602198600769 secnds ---\n",
      "Dataframe WORLD_DB_ALL successfully save in database covid-19-dev in MongoDB. Time: --- 37.461220502853394 secnds ---\n"
     ]
    }
   ],
   "source": [
    "#--- STEP4 ---: Calculate KPIs (output = DB_ALL)\n",
    "start_time = time.time()\n",
    "#-> Referentials\n",
    "ref_tech = pd.read_excel(input_folder + 'REF_WSR.xlsx', sheet_name = 'PARAM_ALL')\n",
    "\n",
    "def step4(df,ref):\n",
    "    #-- Calc Active + Ratio\n",
    "    df['ACTIVE_CASES'] = df['CONFIRMED'] - df['DEATHS'] - df['RECOVERED']\n",
    "    df.loc[:,'DEATHS_RATIO'] = 0\n",
    "    df.loc[df['CONFIRMED'] != 0, 'DEATHS_RATIO'] = df['DEATHS'] / df['CONFIRMED'] * 100\n",
    "    df.loc[:,'RECOVERED_RATIO'] = 0\n",
    "    df.loc[df['CONFIRMED'] != 0, 'RECOVERED_RATIO'] = df['RECOVERED'] / df['CONFIRMED'] * 100\n",
    "    \n",
    "    #-- Melt KPI in rows\n",
    "    cols_to_keep = ['ENTITY_GROUPS','ENTITY','DATE']\n",
    "    df= df.melt(id_vars=cols_to_keep,value_name='VALUE',var_name='KPI')\n",
    "    indexes = df.loc[df['DATE'] == 0].index\n",
    "    df = df.drop(indexes, axis=0)\n",
    "    \n",
    "    #-- Variation vs last day\n",
    "    #Add fields\n",
    "    df['DATE'] = pd.to_datetime(df['DATE'], format='%m/%d/%y')\n",
    "    df['LAST_DAY'] =  pd.to_datetime(df['DATE'] + timedelta(days=-1))\n",
    "    \n",
    "    #Create new df\n",
    "    cols_to_rename = {\"DATE\": \"LAST_DAY\",'VALUE':'VALUE_D-1'}\n",
    "    df_last = df.drop(['LAST_DAY'], axis=1).rename(index=str, columns=cols_to_rename)\n",
    "    \n",
    "    #Merge day-1\n",
    "    cols_to_merge = ['ENTITY_GROUPS','ENTITY','LAST_DAY','KPI']\n",
    "    df = df.merge(df_last, on=cols_to_merge, how='left').fillna(0).drop(['LAST_DAY'], axis=1)\n",
    "    \n",
    "    #Calc variation in value and %\n",
    "    df['VARV'] = df['VALUE'].astype(float) - df['VALUE_D-1'].astype(float) \n",
    "    df.loc[:,'VARP'] = np.NaN\n",
    "    df.loc[(df['KPI'].isin(['CONFIRMED','DEATHS','RECOVERED','ACTIVE_CASES'])) & (df['VALUE_D-1'] != 0), 'VARP'] = df['VARV'] * 100 / abs(df['VALUE_D-1'])\n",
    "    \n",
    "    #-- Add fields\n",
    "    df['SCENARIO'] = pd.to_datetime(df['DATE'], format='%m/%d/%y').dt.strftime('%d/%m/%Y')\n",
    "    df['DATE_ORDER'] = pd.to_datetime(df['SCENARIO'], format='%d/%m/%Y').dt.strftime('%Y%m%d')\n",
    "    \n",
    "    #-- Units / Precisions / Sentiments \n",
    "    df['KPI'] = df['KPI'].str.replace('_',' ').str.lower().str.capitalize()\n",
    "    df= pd.merge(df,ref, on=['KPI'], how='left')\n",
    "    df.loc[df['KPI'] == 'Deaths ratio', 'KPI'] = \"Fatality Rate\"\n",
    "    df.loc[df['KPI'] == 'Recovered ratio', 'KPI'] = \"Recovery Rate\"\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "db_all = step4(db_conso,ref_tech)\n",
    "print(\"Script execution completed at \" + datetime.now().strftime('%d/%m/%Y %H:%M:%S') + \". Time: --- %s secnds ---\" % (time.time() - start_time))\n",
    "db_all.to_csv(output_folder + 'WORLD_DB_ALL.csv',sep=\";\")\n",
    "# if use_mongo:\n",
    "#     bob.mongo.save_df(db_all,'WORLD_DB_ALL',db_src,True)\n",
    "# db_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script execution completed at 13/04/2020 12:27:09. Time: --- 20.88958191871643 secnds ---\n",
      "Dataframe WORLD_DB_TREND successfully save in database covid-19-dev in MongoDB. Time: --- 101.92262101173401 secnds ---\n"
     ]
    }
   ],
   "source": [
    "#--- STEP5 ---: Transform DB_ALL to trend data (output = DB_TREND)\n",
    "start_time = time.time()\n",
    "#-> Referentials\n",
    "ref_tech = pd.read_excel(input_folder + 'REF_WSR.xlsx', sheet_name = 'PARAM_TREND')\n",
    "\n",
    "def step5(df,ref):\n",
    "    #-- Melt CALC in rows\n",
    "    cols_to_keep = ['ENTITY_GROUPS','ENTITY','SCENARIO','DATE','DATE_ORDER','KPI']\n",
    "    df= df.drop(['VALUE_D-1','UNIT_VALUE','UNIT_VAR','UNIT_VARP','PRECISION_VALUE','PRECISION_VAR','PRECISION_VARP'],axis=1).melt(id_vars=cols_to_keep, value_name='VALUE',var_name='METRIC')\n",
    "    df['DATE_SCENARIO'] = 'Since beginning'\n",
    "        \n",
    "    df_last14 = get_lastdays(df, 14,\"Last 14 days\")\n",
    "    df_last30 = get_lastdays(df, 30,\"Last 30 days\")\n",
    "    df = pd.concat([df,df_last14,df_last30],axis=0)\n",
    "    \n",
    "    #-- Get units and precisions\n",
    "    df= pd.merge(df,ref, on=['KPI','METRIC'], how='left')\n",
    "    df.loc[df['KPI'] == 'Deaths ratio', 'KPI'] = \"Fatality Rate\"\n",
    "    df.loc[df['KPI'] == 'Recovered ratio', 'KPI'] = \"Recovery Rate\"\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "db_trend = step5(db_all,ref_tech)\n",
    "print(\"Script execution completed at \" + datetime.now().strftime('%d/%m/%Y %H:%M:%S') + \". Time: --- %s secnds ---\" % (time.time() - start_time))\n",
    "db_trend.to_csv(output_folder + 'WORLD_DB_TREND.csv',sep=\";\")\n",
    "# if use_mongo:\n",
    "#     bob.mongo.save_df(db_trend,'WORLD_DB_TREND',db_src,True)\n",
    "# db_trend.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data check 2020-04-12: TimeSeries - Daily\n",
      "Confirmed: 1846670 - 1846680 = -10\n",
      "Deaths: 114085 - 114090 = -5\n",
      "Recovered: 421722 - 421722 = 0\n",
      "Active cases: 1310863 - 1310868 = -5\n",
      "Fatality rate: 6.177876935240189 - 6.178114237442329 = -0.00023730220214002173\n",
      "Recovery rate: 22.836890186118797 - 22.83676652154136 = 0.00012366457743695491\n"
     ]
    }
   ],
   "source": [
    "#--- STEP6 ---: Check and validate data\n",
    "#-> CHECK TIMESERIES\n",
    "#change yesterday format to match with header\n",
    "yesterday_c = yesterday.strftime('%m/%d/%y')\n",
    "if yesterday_c[:1]== '0':\n",
    "    yesterday_c = yesterday_c[1:].replace('/0','/')\n",
    "\n",
    "#check if yesterday exist in df\n",
    "def check_timeseries(date,df,df_name):\n",
    "    if date in df.columns:\n",
    "        check = df.agg({date:'sum'}).fillna(0)[0]\n",
    "    else:\n",
    "        check = 0\n",
    "        print(f'Column {date} does not exist in {df_name}.\\n')\n",
    "    return check\n",
    "    \n",
    "check_ct = check_timeseries(yesterday_c,cConfirmed, 'CONFIRMED') + check_timeseries(yesterday_c,cConfirmedUS, 'CONFIRMED')\n",
    "check_dt = check_timeseries(yesterday_c,cDeath, 'DEATHS') + check_timeseries(yesterday_c,cDeathUS, 'DEATHS')\n",
    "check_rt = check_timeseries(yesterday_c,cRecovery, 'RECOVERED')\n",
    "check_at = check_ct - check_dt - check_rt\n",
    "if check_ct != 0:\n",
    "    check_frt = check_dt / check_ct * 100\n",
    "    check_rrt = check_rt / check_ct * 100\n",
    "else:\n",
    "    check_frt = 0\n",
    "    check_rrt = 0\n",
    "\n",
    "#-> CHECK DAILY\n",
    "#check if yesterday exist in df\n",
    "def check_timeseries(date_check,df,variable):\n",
    "    if len(df) != 0 and \"Date\" in df.columns and variable in df.columns and type(date_check) is date:\n",
    "        check = df[df['Date'] == date_check.strftime('%m/%d/%y')].agg({variable:'sum'})[0]\n",
    "    else:\n",
    "        check = 0\n",
    "        print(f'No data in Daily\\n')\n",
    "    return check\n",
    "\n",
    "check_cd = check_timeseries(yesterday,cDaily,'Confirmed')\n",
    "check_dd = check_timeseries(yesterday,cDaily,'Deaths')\n",
    "check_rd = check_timeseries(yesterday,cDaily,'Recovered')\n",
    "check_ad = check_cd - check_dd - check_rd\n",
    "if check_cd != 0:\n",
    "    check_frd = check_dd / check_cd * 100\n",
    "    check_rrd = check_rd / check_cd * 100\n",
    "else:\n",
    "    check_frd = 0\n",
    "    check_rrd = 0\n",
    "    \n",
    "#-- CHECK : TIMESERIES vs DAILY\n",
    "check_c = check_ct - check_cd\n",
    "check_d = check_dt - check_dd\n",
    "check_r = check_rt - check_rd\n",
    "check_a = check_at - check_ad\n",
    "check_fr = check_frt - check_frd\n",
    "check_rr = check_rrt - check_rrd\n",
    "\n",
    "print(f'Data check {yesterday}: TimeSeries - Daily')\n",
    "print(f'Confirmed: {check_ct} - {check_cd} = {check_c}')\n",
    "print(f'Deaths: {check_dt} - {check_dd} = {check_d}')\n",
    "print(f'Recovered: {check_rt} - {check_rd} = {check_r}')\n",
    "print(f'Active cases: {check_at} - {check_ad} = {check_a}')\n",
    "print(f'Fatality rate: {check_frt} - {check_frd} = {check_fr}')\n",
    "print(f'Recovery rate: {check_rrt} - {check_rrd} = {check_rr}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
