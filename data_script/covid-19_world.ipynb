{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Source COVID World updated everyday at 2:00 AM : </b> <br>\n",
    "https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run __init__.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP1 ---: Get data source & cleaning\n",
    "START_TIME = time.time()\n",
    "\n",
    "# -- GITHUB : TIMESERIES (CSSEGISandData/COVID-19/csse_covid_19_time_series)\n",
    "# -> WORLD_CONFIRMED\n",
    "URLCONFIRMED = ('https://raw.githubusercontent.com/CSSEGISandData/COVID-19'\n",
    "                '/master/csse_covid_19_data/csse_covid_19_time_series'\n",
    "                '/time_series_covid19_confirmed_global.csv')\n",
    "CCONFIRMED = check_url(URLCONFIRMED)\n",
    "if USE_MONGO:\n",
    "    bob.mongo.save_df(CCONFIRMED, 'WORLD_CONFIRMED', DB_SRC, True)\n",
    "CCONFIRMED.to_csv(os.path.join(OUTPUT_FOLDER, 'WORLD_CONFIRMED.csv'), sep=\";\")\n",
    "\n",
    "# -> WORLD_DEATHS\n",
    "URLDEATH = ('https://raw.githubusercontent.com/CSSEGISandData/COVID-19'\n",
    "            '/master/csse_covid_19_data/csse_covid_19_time_series'\n",
    "            '/time_series_covid19_deaths_global.csv')\n",
    "CDEATH = check_url(URLDEATH)\n",
    "if USE_MONGO:\n",
    "    bob.mongo.save_df(CDEATH, 'WORLD_DEATHS', DB_SRC, True)\n",
    "CDEATH.to_csv(os.path.join(OUTPUT_FOLDER, 'WORLD_DEATHS.csv'), sep=\";\")\n",
    "\n",
    "# -> WORLD_RECOVERED\n",
    "URLRECOVERY = ('https://raw.githubusercontent.com/CSSEGISandData/COVID-19'\n",
    "               '/master/csse_covid_19_data/csse_covid_19_time_series'\n",
    "               '/time_series_covid19_recovered_global.csv')\n",
    "CRECOVERY = check_url(URLRECOVERY)\n",
    "if USE_MONGO:\n",
    "    bob.mongo.save_df(CRECOVERY, 'WORLD_RECOVERED', DB_SRC, True)\n",
    "CRECOVERY.to_csv(os.path.join(OUTPUT_FOLDER, 'WORLD_RECOVERED.csv'), sep=\";\")\n",
    "\n",
    "# -> US_CONFIRMED\n",
    "URLCONFIRMEDUS = ('https://raw.githubusercontent.com/CSSEGISandData/COVID-19'\n",
    "                  '/master/csse_covid_19_data/csse_covid_19_time_series'\n",
    "                  '/time_series_covid19_confirmed_US.csv')\n",
    "CCONFIRMEDUS = check_url(URLCONFIRMEDUS)\n",
    "if USE_MONGO:\n",
    "    bob.mongo.save_df(CCONFIRMED, 'US_CONFIRMED', DB_SRC, True)\n",
    "CCONFIRMEDUS.to_csv(os.path.join(OUTPUT_FOLDER, 'US_CONFIRMED.csv'), sep=\";\")\n",
    "\n",
    "# -> US_DEATHS\n",
    "URLDEATHUS = ('https://raw.githubusercontent.com/CSSEGISandData/COVID-19'\n",
    "              '/master/csse_covid_19_data/csse_covid_19_time_series'\n",
    "              '/time_series_covid19_deaths_US.csv')\n",
    "CDEATHUS = check_url(URLDEATHUS)\n",
    "if USE_MONGO:\n",
    "    bob.mongo.save_df(CDEATH, 'US_DEATHS', DB_SRC, True)\n",
    "CDEATHUS.to_csv(os.path.join(OUTPUT_FOLDER, 'US_DEATHS.csv'), sep=\";\")\n",
    "\n",
    "# -- GITHUB : DAILY REPORT (CSSEGISandData/COVID-19/csse_covid_19_time_series)\n",
    "# -> DAILY\n",
    "DATE_INIT = YESTERDAY.strftime('%m-%d-%Y')\n",
    "DATE_END = YESTERDAY.strftime('%m-%d-%Y')\n",
    "DATES = pd.date_range(start=DATE_INIT,\n",
    "                      end=DATE_END,\n",
    "                      freq='D').strftime('%m-%d-%Y').values.tolist()\n",
    "FILTER_DATES = pd.date_range(start=DATE_INIT,\n",
    "                             end=DATE_END,\n",
    "                             freq='D').strftime('%m/%d/%y').values.tolist()\n",
    "\n",
    "CDAILY = pd.DataFrame()\n",
    "for each_date in DATES:\n",
    "    url_daily = ('https://raw.githubusercontent.com/CSSEGISandData/COVID-19'\n",
    "                 '/master/csse_covid_19_data/csse_covid_19_daily_reports'\n",
    "                 f'/{each_date}.csv')\n",
    "    cols_to_rename = {\"Country/Region\": \"Country_Region\",\n",
    "                      \"Province/State\": \"Province_State\",\n",
    "                      \"Long_\": \"Long\"}\n",
    "    tmp_df = check_url(url_daily).rename(index=str, columns=cols_to_rename)\n",
    "    if tmp_df:\n",
    "        tmp_df['Confirmed'] = tmp_df['Confirmed'].fillna(0).astype(int)\n",
    "        tmp_df['Deaths'] = tmp_df['Deaths'].fillna(0).astype(int)\n",
    "        tmp_df['Recovered'] = tmp_df['Recovered'].fillna(0).astype(int)\n",
    "        tmp_df['Date'] = pd.to_datetime(each_date,\n",
    "                                        format='%m-%d-%Y').strftime('%m/%d/%y')\n",
    "        CDAILY = CDAILY.append(tmp_df)\n",
    "print(\"Script execution completed at \"\n",
    "      f\"{datetime.now().strftime('%d/%m/%Y %H:%M:%S')}. \"\n",
    "      f\"Time: --- {time.time() - START_TIME} secnds ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pd.ExcelWriter(output_folder +\n",
    "#                     'WSR_datasource_' +\n",
    "#                     yesterday.strftime('%Y%m%d') + '.xlsx') as writer:\n",
    "#     cConfirmed.to_excel(writer,sheet_name =\"WORLD_CONFIRMED\")\n",
    "#     cDeath.to_excel(writer,sheet_name =\"WORLD_DEATHS\")\n",
    "#     cRecovery.to_excel(writer,sheet_name =\"WORLD_RECOVERED\")\n",
    "#     cDaily.to_excel(writer,sheet_name =\"WORLD_DAILY\")\n",
    "#     cConfirmedUS.to_excel(writer,sheet_name =\"US_CONFIRMED\")\n",
    "#     cDeathUS.to_excel(writer,sheet_name =\"US_DEATHS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Melt data source from timeseries to have date in rows\n",
    "def melt_cleaning(dataf, variable):\n",
    "    \"\"\"\n",
    "    Melt data source from timeseries to have date in rows\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if 'Province/State' and 'Country/Region' in dataf.columns:\n",
    "            # Add Country/Region to Province/State if empty\n",
    "            dataf = dataf.fillna(0)\n",
    "            dataf.loc[dataf['Province/State'] == 0,\n",
    "                      'Province/State'] = dataf['Country/Region']\n",
    "            # Melt and group data\n",
    "            cols_to_keep = ['Country/Region', 'Province/State']\n",
    "            cols_to_group = ['Country/Region', 'Province/State', 'Date']\n",
    "            dataf = (dataf.melt(id_vars=cols_to_keep,\n",
    "                                value_name=variable,\n",
    "                                var_name='Date').\n",
    "                     groupby(cols_to_group,\n",
    "                             as_index=False).agg({variable: 'sum'}))\n",
    "            # Data format\n",
    "            dataf['Date'] = (pd.\n",
    "                             to_datetime(dataf['Date'],\n",
    "                                         format='%m/%d/%y').\n",
    "                             dt.strftime('%m/%d/%y'))\n",
    "\n",
    "            if variable == 'Confirmed':\n",
    "                dataf['Deaths'] = 0\n",
    "                dataf['Recovered'] = 0\n",
    "            elif variable == 'Deaths':\n",
    "                dataf['Confirmed'] = 0\n",
    "                dataf['Recovered'] = 0\n",
    "            else:\n",
    "                dataf['Confirmed'] = 0\n",
    "                dataf['Deaths'] = 0\n",
    "        else:\n",
    "            dataf = pd.DataFrame()\n",
    "    except Exception as expt:\n",
    "        dataf = pd.DataFrame()\n",
    "        print(expt.__doc__)\n",
    "        print(str(expt))\n",
    "    return dataf\n",
    "\n",
    "\n",
    "def cols_cleaning(dataf, cols_to_drop):\n",
    "    \"\"\"\n",
    "    doc string\n",
    "    \"\"\"\n",
    "    for each_col in cols_to_drop:\n",
    "        if each_col in dataf.columns:\n",
    "            dataf = dataf.drop(each_col, axis=1)\n",
    "\n",
    "    cols_to_rename = {\"Province_State\": \"Province/State\",\n",
    "                      \"Country_Region\": \"Country/Region\"}\n",
    "    dataf = dataf.rename(index=str, columns=cols_to_rename)\n",
    "    return dataf\n",
    "\n",
    "\n",
    "# Timeseries : World Confirmed\n",
    "COLS_TO_DROP = ['Lat', 'Long']\n",
    "CCONFIRMED = CCONFIRMED[CCONFIRMED['Country/Region'] != 'US']\n",
    "DF_CW = melt_cleaning(cols_cleaning(CCONFIRMED, COLS_TO_DROP), 'Confirmed')\n",
    "\n",
    "# Timeseries : World Deaths\n",
    "COLS_TO_DROP = ['Lat', 'Long']\n",
    "CDEATH = CDEATH[CDEATH['Country/Region'] != 'US']\n",
    "DF_DW = melt_cleaning(cols_cleaning(CDEATH, COLS_TO_DROP), 'Deaths')\n",
    "\n",
    "# Timeseries : World Recovered\n",
    "COLS_TO_DROP = ['Lat', 'Long']\n",
    "DF_RW = melt_cleaning(cols_cleaning(CRECOVERY, COLS_TO_DROP), 'Recovered')\n",
    "\n",
    "# Timeseries : US Confirmed\n",
    "COLS_TO_DROP = ['UID', 'iso2', 'iso3', 'code3', 'FIPS',\n",
    "                'Admin2', 'Lat', 'Long_', 'Combined_Key']\n",
    "DF_CUS = melt_cleaning(cols_cleaning(CCONFIRMEDUS, COLS_TO_DROP), 'Confirmed')\n",
    "\n",
    "# Timeseries : US Deaths\n",
    "COLS_TO_DROP = ['UID', 'iso2', 'iso3', 'code3', 'FIPS',\n",
    "                'Admin2', 'Lat', 'Long_', 'Combined_Key', 'Population']\n",
    "DF_DUS = melt_cleaning(cols_cleaning(CDEATHUS, COLS_TO_DROP), 'Deaths')\n",
    "\n",
    "# Manual input\n",
    "DF_MANUAL = pd.read_excel(os.path.join(INPUT_FOLDER, 'REF_WSR.xlsx'),\n",
    "                          sheet_name='DATA_INPUT').fillna(0)\n",
    "DF_MANUAL['Date'] = pd.to_datetime(DF_MANUAL['Date'],\n",
    "                                   format='%Y%m%d').dt.strftime('%m/%d/%y')\n",
    "# DF_MANUAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP2 ---: Create DB with data source (output = DB_CONCAT)\n",
    "START_TIME = time.time()\n",
    "\n",
    "\n",
    "# --- step ---: test\n",
    "def step2(df1, df2, df3, df4, df5, df6):\n",
    "    \"\"\"\n",
    "    Create DB with data source (output => DB_CONCAT)\n",
    "    \"\"\"\n",
    "    # -- Merge timeseries\n",
    "    cols_to_group = ['Country/Region', 'Province/State', 'Date']\n",
    "    dataf = pd.concat([df1, df2, df3, df4, df5, df6],\n",
    "                      axis=0).groupby(cols_to_group,\n",
    "                                      as_index=False).agg({'Confirmed': 'sum',\n",
    "                                                           'Deaths': 'sum',\n",
    "                                                           'Recovered': 'sum'})\n",
    "    return dataf.reset_index(drop=True)\n",
    "\n",
    "\n",
    "DB_CONCAT = step2(DF_CW, DF_DW, DF_RW, DF_CUS, DF_DUS, DF_MANUAL)\n",
    "print(\"Script execution completed at \"\n",
    "      f\"{datetime.now().strftime('%d/%m/%Y %H:%M:%S')}. \"\n",
    "      f\"Time: --- {time.time() - START_TIME} secnds ---\")\n",
    "DB_CONCAT.to_csv(os.path.join(OUTPUT_FOLDER, 'WORLD_DB_CONCAT.csv'), sep=\";\")\n",
    "# bob.mongo.save_df(DB_CONCAT, 'WORLD_DB_CONCAT', DB_SRC, True)\n",
    "# DB_CONCAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Check if date is updated\n",
    "DF_CHECK = DB_CONCAT[DB_CONCAT['Date'] == YESTERDAY.strftime('%m/%d/%y')]\n",
    "if not DF_CHECK:\n",
    "    print(f'Data not updated ! DateTime : {now}')\n",
    "    %stop\n",
    "else:\n",
    "    print(f\"Data updated, {YESTERDAY} exits in WORLD_DB_CONCAT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP3 ---: Consolidate and enrich data (output = DB_CONSO)\n",
    "START_TIME = time.time()\n",
    "# -> Referentials\n",
    "REF_CONTINENT = pd.read_excel(os.path.join(INPUT_FOLDER, 'REF_WSR.xlsx'),\n",
    "                              sheet_name='REF_CONTINENT')\n",
    "\n",
    "\n",
    "def step3(dataf, ref):\n",
    "    \"\"\"\n",
    "    doc string\n",
    "    \"\"\"\n",
    "    dataf.columns = dataf.columns.str.upper()\n",
    "    cols_to_rename = {\"COUNTRY/REGION\": \"ENTITY_GROUPS\",\n",
    "                      \"PROVINCE/STATE\": \"ENTITY\"}\n",
    "    dataf = dataf.rename(index=str, columns=cols_to_rename)\n",
    "\n",
    "    cols_to_rename = {\"ENTITY_GROUPS\": \"ENTITY\", \"CONTINENT\": \"ENTITY_GROUPS\"}\n",
    "    df_con = (pd.merge(dataf.drop(['ENTITY'], axis=1),\n",
    "                       ref.drop(['WORLDMAP'], axis=1),\n",
    "                       left_on=['ENTITY_GROUPS'],\n",
    "                       right_on=['COUNTRY_REGION'],\n",
    "                       how='left').\n",
    "              rename(index=str, columns=cols_to_rename).\n",
    "              drop(['COUNTRY_REGION'], axis=1).\n",
    "              fillna(0))\n",
    "    df_con.loc[df_con['ENTITY_GROUPS'] == 0,\n",
    "               'ENTITY_GROUPS'] = 'To be affected'\n",
    "\n",
    "    cols_to_rename = {\"ENTITY_GROUPS\": \"ENTITY\"}\n",
    "    df_ww = df_con.copy().drop(['ENTITY'],\n",
    "                               axis=1).rename(index=str,\n",
    "                                              columns=cols_to_rename)\n",
    "    df_ww['ENTITY_GROUPS'] = 'WORLDWIDE'\n",
    "\n",
    "    cols_to_rename = {\"ENTITY_GROUPS\": \"ENTITY\"}\n",
    "    df_tt = df_ww.copy().drop(['ENTITY'],\n",
    "                              axis=1).rename(index=str,\n",
    "                                             columns=cols_to_rename)\n",
    "    df_tt['ENTITY_GROUPS'] = 'WORLDWIDE'\n",
    "\n",
    "    dataf = pd.concat([dataf, df_con, df_ww, df_tt], axis=0)\n",
    "    dataf = dataf.groupby(['ENTITY_GROUPS', 'ENTITY', 'DATE'],\n",
    "                          as_index=False).agg({'CONFIRMED': 'sum',\n",
    "                                               'DEATHS': 'sum',\n",
    "                                               'RECOVERED': 'sum'})\n",
    "    return dataf.reset_index(drop=True)\n",
    "\n",
    "\n",
    "DB_CONSO = step3(DB_CONCAT, REF_CONTINENT)\n",
    "print(\"Script execution completed at \"\n",
    "      f\"{datetime.now().strftime('%d/%m/%Y %H:%M:%S')}. \"\n",
    "      f\"Time: --- {time.time() - START_TIME} secnds ---\")\n",
    "DB_CONSO.to_csv(os.path.join(OUTPUT_FOLDER + 'WORLD_DB_CONSO.csv'), sep=\";\")\n",
    "# if USE_MONGO:\n",
    "#     bob.mongo.save_df(DB_CONSO, 'WORLD_DB_CONSO', DB_SRC, True)\n",
    "# DB_CONSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP4 ---: Calculate KPIs (output = DB_ALL)\n",
    "START_TIME = time.time()\n",
    "# -> Referentials\n",
    "REF_TECH = pd.read_excel(os.path.join(INPUT_FOLDER, 'REF_WSR.xlsx'),\n",
    "                         sheet_name='PARAM_ALL')\n",
    "\n",
    "\n",
    "def step4(dataf, ref):\n",
    "    \"\"\"\n",
    "    doc string\n",
    "    \"\"\"\n",
    "    # -- Calc Active + Ratio\n",
    "    dataf['ACTIVE_CASES'] = (dataf['CONFIRMED'] -\n",
    "                             dataf['DEATHS'] -\n",
    "                             dataf['RECOVERED'])\n",
    "    dataf.loc[:, 'DEATHS_RATIO'] = 0\n",
    "    dataf.loc[dataf['CONFIRMED'] != 0,\n",
    "              'DEATHS_RATIO'] = dataf['DEATHS'] / dataf['CONFIRMED'] * 100\n",
    "    dataf.loc[:, 'RECOVERED_RATIO'] = 0\n",
    "    dataf.loc[dataf['CONFIRMED'] != 0,\n",
    "              'RECOVERED_RATIO'] = (dataf['RECOVERED'] /\n",
    "                                    dataf['CONFIRMED'] *\n",
    "                                    100)\n",
    "\n",
    "    # -- Melt KPI in rows\n",
    "    cols_to_keep = ['ENTITY_GROUPS', 'ENTITY', 'DATE']\n",
    "    dataf = dataf.melt(id_vars=cols_to_keep,\n",
    "                       value_name='VALUE',\n",
    "                       var_name='KPI')\n",
    "    indexes = dataf.loc[dataf['DATE'] == 0].index\n",
    "    dataf = dataf.drop(indexes, axis=0)\n",
    "\n",
    "    # -- Variation vs last day\n",
    "    # Add fields\n",
    "    dataf['DATE'] = pd.to_datetime(dataf['DATE'], format='%m/%d/%y')\n",
    "    dataf['LAST_DAY'] = pd.to_datetime(dataf['DATE'] + timedelta(days=-1))\n",
    "\n",
    "    # Create new df\n",
    "    cols_to_rename = {\"DATE\": \"LAST_DAY\", 'VALUE': 'VALUE_D-1'}\n",
    "    df_last = dataf.drop(['LAST_DAY'], axis=1).rename(index=str,\n",
    "                                                      columns=cols_to_rename)\n",
    "\n",
    "    # Merge day-1\n",
    "    cols_to_merge = ['ENTITY_GROUPS', 'ENTITY', 'LAST_DAY', 'KPI']\n",
    "    dataf = dataf.merge(df_last,\n",
    "                        on=cols_to_merge,\n",
    "                        how='left').fillna(0).drop(['LAST_DAY'],\n",
    "                                                   axis=1)\n",
    "\n",
    "    # Calc variation in value and %\n",
    "    dataf['VARV'] = (dataf['VALUE'].astype(float) -\n",
    "                     dataf['VALUE_D-1'].astype(float))\n",
    "    dataf.loc[:, 'VARP'] = np.NaN\n",
    "    dataf.loc[(dataf['KPI'].\n",
    "               isin(['CONFIRMED',\n",
    "                     'DEATHS',\n",
    "                     'RECOVERED',\n",
    "                     'ACTIVE_CASES'])) &\n",
    "              dataf['VALUE_D-1'] != 0, 'VARP'] = (dataf['VARV'] *\n",
    "                                                  100 /\n",
    "                                                  abs(dataf['VALUE_D-1']))\n",
    "\n",
    "    # -- Add fields\n",
    "    dataf['SCENARIO'] = (pd.to_datetime(dataf['DATE'],\n",
    "                                        format='%m/%d/%y').\n",
    "                         dt.strftime('%d/%m/%Y'))\n",
    "    dataf['DATE_ORDER'] = (pd.to_datetime(dataf['SCENARIO'],\n",
    "                                          format='%d/%m/%Y').\n",
    "                           dt.strftime('%Y%m%d'))\n",
    "\n",
    "    # -- Units / Precisions / Sentiments\n",
    "    dataf['KPI'] = dataf['KPI'].str.replace('_',\n",
    "                                            ' ').str.lower().str.capitalize()\n",
    "    dataf = pd.merge(dataf, ref, on=['KPI'], how='left')\n",
    "    dataf.loc[dataf['KPI'] == 'Deaths ratio', 'KPI'] = \"Fatality Rate\"\n",
    "    dataf.loc[dataf['KPI'] == 'Recovered ratio', 'KPI'] = \"Recovery Rate\"\n",
    "    return dataf.reset_index(drop=True)\n",
    "\n",
    "\n",
    "DB_ALL = step4(DB_CONSO, REF_TECH)\n",
    "print(\"Script execution completed at \"\n",
    "      f\"{datetime.now().strftime('%d/%m/%Y %H:%M:%S')}. \"\n",
    "      f\"Time: --- {time.time() - START_TIME} secnds ---\")\n",
    "DB_ALL.to_csv(os.path.join(OUTPUT_FOLDER, 'WORLD_DB_ALL.csv'), sep=\";\")\n",
    "# if USE_MONGO:\n",
    "#     bob.mongo.save_df(DB_ALL, 'WORLD_DB_ALL', DB_SRC, True)\n",
    "# DB_ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP5 ---: Transform DB_ALL to trend data (output = DB_TREND)\n",
    "START_TIME = time.time()\n",
    "# -> Referentials\n",
    "REF_TECH = pd.read_excel(os.path.join(INPUT_FOLDER, 'REF_WSR.xlsx'),\n",
    "                         sheet_name='PARAM_TREND')\n",
    "\n",
    "\n",
    "def step5(dataf, ref):\n",
    "    \"\"\"\n",
    "    doc string\n",
    "    \"\"\"\n",
    "    # -- Melt CALC in rows\n",
    "    cols_to_keep = ['ENTITY_GROUPS', 'ENTITY', 'SCENARIO',\n",
    "                    'DATE', 'DATE_ORDER', 'KPI']\n",
    "    dataf = dataf.drop(['VALUE_D-1', 'UNIT_VALUE', 'UNIT_VAR', 'UNIT_VARP',\n",
    "                        'PRECISION_VALUE', 'PRECISION_VAR', 'PRECISION_VARP'],\n",
    "                       axis=1).melt(id_vars=cols_to_keep,\n",
    "                                    value_name='VALUE',\n",
    "                                    var_name='METRIC')\n",
    "    dataf['DATE_SCENARIO'] = 'Since beginning'\n",
    "\n",
    "    df_last14 = get_lastdays(dataf, 14, \"Last 14 days\")\n",
    "    df_last30 = get_lastdays(dataf, 30, \"Last 30 days\")\n",
    "    dataf = pd.concat([dataf, df_last14, df_last30], axis=0)\n",
    "\n",
    "    # -- Get units and precisions\n",
    "    dataf = pd.merge(dataf, ref, on=['KPI', 'METRIC'], how='left')\n",
    "    dataf.loc[dataf['KPI'] == 'Deaths ratio', 'KPI'] = \"Fatality Rate\"\n",
    "    dataf.loc[dataf['KPI'] == 'Recovered ratio', 'KPI'] = \"Recovery Rate\"\n",
    "    dataf = dataf.reset_index(drop=True)\n",
    "    return dataf\n",
    "\n",
    "\n",
    "DB_TREND = step5(DB_ALL, REF_TECH)\n",
    "print(\"Script execution completed at \"\n",
    "      f\"{datetime.now().strftime('%d/%m/%Y %H:%M:%S')}. \"\n",
    "      f\"Time: --- {time.time() - START_TIME} secnds ---\")\n",
    "DB_TREND.to_csv(os.path.join(OUTPUT_FOLDER, 'WORLD_DB_TREND.csv'), sep=\";\")\n",
    "# if use_mongo:\n",
    "#     bob.mongo.save_df(db_trend,'WORLD_DB_TREND',db_src,True)\n",
    "# db_trend.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP6 ---: Check and validate data\n",
    "# -> CHECK TIMESERIES\n",
    "# change YESTERDAY format to match with header\n",
    "YESTERDAY_C = YESTERDAY.strftime('%m/%d/%y')\n",
    "if YESTERDAY_C[:1] == '0':\n",
    "    YESTERDAY_C = YESTERDAY_C[1:].replace('/0', '/')\n",
    "\n",
    "\n",
    "# check if YESTERDAY exist in df\n",
    "def check_timeseries(date_in, dataf, df_name):\n",
    "    \"\"\"\n",
    "    doc string\n",
    "    \"\"\"\n",
    "    if date_in in dataf.columns:\n",
    "        check = dataf.agg({date_in: 'sum'}).fillna(0)[0]\n",
    "    else:\n",
    "        check = 0\n",
    "        print(f'Column {date_in} does not exist in {df_name}.\\n')\n",
    "    return check\n",
    "\n",
    "\n",
    "CHECK_CT = check_timeseries(YESTERDAY_C,\n",
    "                            CCONFIRMED,\n",
    "                            'CONFIRMED') + check_timeseries(YESTERDAY_C,\n",
    "                                                            CCONFIRMEDUS,\n",
    "                                                            'CONFIRMED')\n",
    "CHECK_DT = check_timeseries(YESTERDAY_C,\n",
    "                            CDEATH,\n",
    "                            'DEATHS') + check_timeseries(YESTERDAY_C,\n",
    "                                                         CDEATHUS,\n",
    "                                                         'DEATHS')\n",
    "CHECK_RT = check_timeseries(YESTERDAY_C, CRECOVERY, 'RECOVERED')\n",
    "CHECK_AT = CHECK_CT - CHECK_DT - CHECK_RT\n",
    "if CHECK_CT != 0:\n",
    "    CHECK_FRT = CHECK_DT / CHECK_CT * 100\n",
    "    CHECK_RRT = CHECK_RT / CHECK_CT * 100\n",
    "else:\n",
    "    CHECK_FRT = 0\n",
    "    CHECK_RRT = 0\n",
    "\n",
    "\n",
    "# -> CHECK DAILY\n",
    "# check if YESTERDAY exist in df\n",
    "def check_timeseries(date_check, dataf, variable):\n",
    "    \"\"\"\n",
    "    doc string\n",
    "    \"\"\"\n",
    "    if (dataf and\n",
    "            \"Date\" in dataf.columns and\n",
    "            variable in dataf.columns and\n",
    "            isinstance(date_check, date)):\n",
    "        check = (dataf[dataf['Date'] == date_check.strftime('%m/%d/%y')].\n",
    "                 agg({variable: 'sum'})[0])\n",
    "    else:\n",
    "        check = 0\n",
    "        print(f'No data in Daily\\n')\n",
    "    return check\n",
    "\n",
    "\n",
    "CHECK_CD = check_timeseries(YESTERDAY, CDAILY, 'Confirmed')\n",
    "CHECK_DD = check_timeseries(YESTERDAY, CDAILY, 'Deaths')\n",
    "CHECK_RD = check_timeseries(YESTERDAY, CDAILY, 'Recovered')\n",
    "CHECK_AD = CHECK_CD - CHECK_DD - CHECK_RD\n",
    "if CHECK_CD:\n",
    "    CHECK_FRD = CHECK_DD / CHECK_CD * 100\n",
    "    CHECK_RRD = CHECK_RD / CHECK_CD * 100\n",
    "else:\n",
    "    CHECK_FRD = 0\n",
    "    CHECK_RRD = 0\n",
    "\n",
    "# -- CHECK : TIMESERIES vs DAILY\n",
    "CHECK_C = CHECK_CT - CHECK_CD\n",
    "CHECK_D = CHECK_DT - CHECK_DD\n",
    "CHECK_R = CHECK_RT - CHECK_RD\n",
    "CHECK_A = CHECK_AT - CHECK_AD\n",
    "CHECK_FR = CHECK_FRT - CHECK_FRD\n",
    "CHECK_RR = CHECK_RRT - CHECK_RRD\n",
    "\n",
    "print(f'Data check {YESTERDAY}: TimeSeries - Daily')\n",
    "print(f'Confirmed: {CHECK_CT} - {CHECK_CD} = {CHECK_C}')\n",
    "print(f'Deaths: {CHECK_DT} - {CHECK_DD} = {CHECK_D}')\n",
    "print(f'Recovered: {CHECK_RT} - {CHECK_RD} = {CHECK_R}')\n",
    "print(f'Active cases: {CHECK_AT} - {CHECK_AD} = {CHECK_A}')\n",
    "print(f'Fatality rate: {CHECK_FRT} - {CHECK_FRD} = {CHECK_FR}')\n",
    "print(f'Recovery rate: {CHECK_RRT} - {CHECK_RRD} = {CHECK_RR}')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
