{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Source COVID World updated everyday at 2:00 AM : </b> <br> \n",
    "https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_time_series <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to MongoDB\n"
     ]
    }
   ],
   "source": [
    "# %run __init__.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe WORLD_CONFIRMED successfully save in database covid-19-dev in MongoDB. Time: --- 0.07125282287597656 secnds ---\n",
      "Dataframe WORLD_DEATHS successfully save in database covid-19-dev in MongoDB. Time: --- 0.06275224685668945 secnds ---\n",
      "Dataframe WORLD_RECOVERED successfully save in database covid-19-dev in MongoDB. Time: --- 0.06189537048339844 secnds ---\n",
      "Dataframe US_CONFIRMED successfully save in database covid-19-dev in MongoDB. Time: --- 0.06534671783447266 secnds ---\n",
      "Dataframe US_DEATHS successfully save in database covid-19-dev in MongoDB. Time: --- 0.06759905815124512 secnds ---\n",
      "Script execution completed at 28/04/2020 06:45:41. Time: --- 0.7787654399871826 secnds ---\n"
     ]
    }
   ],
   "source": [
    "#--- STEP1 ---: Get data source & cleaning\n",
    "start_time = time.time()\n",
    "\n",
    "#-- GITHUB : TIMESERIES (CSSEGISandData/COVID-19/csse_covid_19_time_series)\n",
    "#-> WORLD_CONFIRMED\n",
    "urlConfirmed = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv'\n",
    "cConfirmed=check_url(urlConfirmed)\n",
    "if use_mongo:\n",
    "    bob.mongo.save_df(cConfirmed,'WORLD_CONFIRMED',db_src,True)\n",
    "cConfirmed.to_csv(output_folder + 'WORLD_CONFIRMED.csv',sep=\";\") \n",
    "\n",
    "#-> WORLD_DEATHS\n",
    "urlDeath = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv'\n",
    "cDeath=check_url(urlDeath)\n",
    "if use_mongo:\n",
    "    bob.mongo.save_df(cDeath,'WORLD_DEATHS',db_src,True)\n",
    "cDeath.to_csv(output_folder + 'WORLD_DEATHS.csv',sep=\";\") \n",
    "\n",
    "#-> WORLD_RECOVERED\n",
    "urlRecovery = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_recovered_global.csv'\n",
    "cRecovery=check_url(urlRecovery)\n",
    "if use_mongo:\n",
    "    bob.mongo.save_df(cRecovery,'WORLD_RECOVERED',db_src,True)\n",
    "cRecovery.to_csv(output_folder + 'WORLD_RECOVERED.csv',sep=\";\") \n",
    "\n",
    "#-> US_CONFIRMED\n",
    "urlConfirmedUS = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv'\n",
    "cConfirmedUS=check_url(urlConfirmedUS)\n",
    "if use_mongo:\n",
    "    bob.mongo.save_df(cConfirmed,'US_CONFIRMED',db_src,True)\n",
    "cConfirmedUS.to_csv(output_folder + 'US_CONFIRMED.csv',sep=\";\") \n",
    "\n",
    "#-> US_DEATHS\n",
    "urlDeathUS = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_US.csv'\n",
    "cDeathUS=check_url(urlDeathUS)\n",
    "if use_mongo:\n",
    "    bob.mongo.save_df(cDeath,'US_DEATHS',db_src,True)\n",
    "cDeathUS.to_csv(output_folder + 'US_DEATHS.csv',sep=\";\") \n",
    "\n",
    "#-- GITHUB : DAILY REPORT (CSSEGISandData/COVID-19/csse_covid_19_time_series)\n",
    "#-> DAILY\n",
    "date_init = yesterday.strftime('%m-%d-%Y')\n",
    "date_end = yesterday.strftime('%m-%d-%Y')\n",
    "dates = pd.date_range(start=date_init,end=date_end, freq='D').strftime('%m-%d-%Y').values.tolist()\n",
    "filter_dates = pd.date_range(start=date_init,end=date_end, freq='D').strftime('%m/%d/%y').values.tolist()\n",
    "\n",
    "cDaily = pd.DataFrame()\n",
    "for d in dates:\n",
    "    urlDaily = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports/' + d + '.csv'\n",
    "    cols_to_rename = {\"Country/Region\": \"Country_Region\",\"Province/State\": \"Province_State\", \"Long_\":\"Long\"}\n",
    "    tmp_df = check_url(urlDaily).rename(index=str, columns=cols_to_rename)\n",
    "    if len(tmp_df) != 0:\n",
    "        tmp_df['Confirmed'] = tmp_df['Confirmed'].fillna(0).astype(int)\n",
    "        tmp_df['Deaths'] = tmp_df['Deaths'].fillna(0).astype(int)\n",
    "        tmp_df['Recovered'] = tmp_df['Recovered'].fillna(0).astype(int)\n",
    "        tmp_df['Date'] = pd.to_datetime(d, format='%m-%d-%Y').strftime('%m/%d/%y')\n",
    "        cDaily = cDaily.append(tmp_df)\n",
    "print(\"Script execution completed at \" + datetime.now().strftime('%d/%m/%Y %H:%M:%S') + \". Time: --- %s secnds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pd.ExcelWriter(output_folder + 'WSR_datasource_' + yesterday.strftime('%Y%m%d') + '.xlsx') as writer:  \n",
    "#     cConfirmed.to_excel(writer,sheet_name =\"WORLD_CONFIRMED\")\n",
    "#     cDeath.to_excel(writer,sheet_name =\"WORLD_DEATHS\")\n",
    "#     cRecovery.to_excel(writer,sheet_name =\"WORLD_RECOVERED\")\n",
    "#     cDaily.to_excel(writer,sheet_name =\"WORLD_DAILY\")\n",
    "#     cConfirmedUS.to_excel(writer,sheet_name =\"US_CONFIRMED\")\n",
    "#     cDeathUS.to_excel(writer,sheet_name =\"US_DEATHS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Melt data source from timeseries to have date in rows\n",
    "def melt_cleaning(df,variable):\n",
    "    try:\n",
    "        if 'Province/State' and 'Country/Region' in df.columns:\n",
    "            #Add Country/Region to Province/State if empty\n",
    "            df = df.fillna(0)\n",
    "            df.loc[df['Province/State'] == 0,'Province/State'] = df['Country/Region']\n",
    "            #Melt and group data\n",
    "            cols_to_keep = ['Country/Region','Province/State']\n",
    "            cols_to_group = ['Country/Region','Province/State','Date']\n",
    "            df= df.melt(id_vars=cols_to_keep,value_name=variable,var_name='Date').groupby(cols_to_group, as_index=False).agg({variable:'sum'})\n",
    "            #Data format\n",
    "            df['Date'] = pd.to_datetime(df['Date'], format='%m/%d/%y').dt.strftime('%m/%d/%y')\n",
    "\n",
    "            if variable == 'Confirmed':\n",
    "                df['Deaths'] = 0\n",
    "                df['Recovered'] = 0\n",
    "            elif variable == 'Deaths':\n",
    "                df['Confirmed'] = 0\n",
    "                df['Recovered'] = 0\n",
    "            else:\n",
    "                df['Confirmed'] = 0\n",
    "                df['Deaths'] = 0\n",
    "        else:\n",
    "            df=pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        df=pd.DataFrame()\n",
    "        print(e.__doc__)\n",
    "        print(str(e))  \n",
    "    return df\n",
    "\n",
    "def cols_cleaning(df,cols_to_drop):\n",
    "    for c in cols_to_drop:\n",
    "        if c in df.columns:\n",
    "            df = df.drop(c,axis=1)\n",
    "            \n",
    "    cols_to_rename = {\"Province_State\": \"Province/State\", \"Country_Region\": \"Country/Region\"}\n",
    "    df = df.rename(index=str, columns=cols_to_rename)\n",
    "    return df       \n",
    "\n",
    "# Timeseries : World Confirmed\n",
    "cols_to_drop = ['Lat','Long']\n",
    "cConfirmed=cConfirmed[cConfirmed['Country/Region'] != 'US']\n",
    "df_cW = melt_cleaning(cols_cleaning(cConfirmed, cols_to_drop),'Confirmed')\n",
    "    \n",
    "# Timeseries : World Deaths\n",
    "cols_to_drop = ['Lat','Long']\n",
    "cDeath=cDeath[cDeath['Country/Region'] != 'US']\n",
    "df_dW = melt_cleaning(cols_cleaning(cDeath, cols_to_drop),'Deaths')\n",
    "    \n",
    "# Timeseries : World Recovered\n",
    "cols_to_drop = ['Lat','Long']\n",
    "df_rW = melt_cleaning(cols_cleaning(cRecovery, cols_to_drop),'Recovered')\n",
    "\n",
    "# Timeseries : US Confirmed\n",
    "cols_to_drop = ['UID','iso2', 'iso3','code3','FIPS','Admin2','Lat','Long_','Combined_Key']\n",
    "df_cUS = melt_cleaning(cols_cleaning(cConfirmedUS, cols_to_drop),'Confirmed')\n",
    "\n",
    "# Timeseries : US Deaths\n",
    "cols_to_drop = ['UID','iso2', 'iso3','code3','FIPS','Admin2','Lat','Long_','Combined_Key','Population']\n",
    "df_dUS = melt_cleaning(cols_cleaning(cDeathUS, cols_to_drop),'Deaths')\n",
    "\n",
    "# Manual input\n",
    "df_manual = pd.read_excel(input_folder + 'REF_WSR.xlsx', sheet_name = 'DATA_INPUT').fillna(0)\n",
    "df_manual['Date'] = pd.to_datetime(df_manual['Date'], format='%Y%m%d').dt.strftime('%m/%d/%y')\n",
    "# df_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script execution completed at 28/04/2020 06:45:42. Time: --- 0.05651998519897461 secnds ---\n"
     ]
    }
   ],
   "source": [
    "#--- STEP2 ---: Create DB with data source (output = DB_CONCAT)\n",
    "start_time = time.time()\n",
    "def step2(df1,df2,df3,df4,df5,df6):\n",
    "    #-- Merge timeseries\n",
    "    cols_to_group = ['Country/Region','Province/State','Date']\n",
    "    df = pd.concat([df1,df2,df3,df4,df5,df6], axis=0).groupby(cols_to_group, as_index=False).agg({'Confirmed':'sum','Deaths':'sum','Recovered':'sum'})\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "db_concat = step2(df_cW,df_dW,df_rW,df_cUS,df_dUS,df_manual)\n",
    "print(\"Script execution completed at \" + datetime.now().strftime('%d/%m/%Y %H:%M:%S') + \". Time: --- %s secnds ---\" % (time.time() - start_time))\n",
    "db_concat.to_csv(output_folder + 'WORLD_DB_CONCAT.csv',sep=\";\")\n",
    "# bob.mongo.save_df(db_concat,'WORLD_DB_CONCAT',db_src,True)\n",
    "# db_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data updated, 2020-04-27 exits in WORLD_DB_CONCAT\n"
     ]
    }
   ],
   "source": [
    "#--- Check if date is updated\n",
    "df_check = db_concat[db_concat['Date'] == yesterday.strftime('%m/%d/%y')]\n",
    "if len(df_check) == 0:\n",
    "    print(f'Data not updated ! DateTime : {now}')\n",
    "    %stop\n",
    "else:\n",
    "    print(f\"Data updated, {yesterday} exits in WORLD_DB_CONCAT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script execution completed at 28/04/2020 06:45:43. Time: --- 0.33562684059143066 secnds ---\n"
     ]
    }
   ],
   "source": [
    "#--- STEP3 ---: Consolidate and enrich data (output = DB_CONSO)\n",
    "start_time = time.time()\n",
    "#-> Referentials\n",
    "ref_continent = pd.read_excel(input_folder + 'REF_WSR.xlsx', sheet_name = 'REF_CONTINENT')\n",
    "def step3(df,ref):\n",
    "    df.columns = df.columns.str.upper()\n",
    "    cols_to_rename = {\"COUNTRY/REGION\": \"ENTITY_GROUPS\", \"PROVINCE/STATE\": \"ENTITY\"}\n",
    "    df = df.rename(index=str, columns=cols_to_rename)\n",
    "    \n",
    "    cols_to_rename = {\"ENTITY_GROUPS\": \"ENTITY\", \"CONTINENT\": \"ENTITY_GROUPS\"}\n",
    "    df_con = pd.merge(df.drop(['ENTITY'],axis=1),ref.drop(['WORLDMAP'],axis=1), left_on=['ENTITY_GROUPS'], right_on=['COUNTRY_REGION'], how='left').rename(index=str, columns=cols_to_rename).drop(['COUNTRY_REGION'],axis=1).fillna(0)\n",
    "    df_con.loc[df_con['ENTITY_GROUPS'] == 0, 'ENTITY_GROUPS'] = 'To be affected'\n",
    "    \n",
    "    cols_to_rename = {\"ENTITY_GROUPS\": \"ENTITY\"}\n",
    "    df_ww = df_con.copy().drop(['ENTITY'],axis=1).rename(index=str, columns=cols_to_rename)\n",
    "    df_ww['ENTITY_GROUPS'] = 'WORLDWIDE'\n",
    "    \n",
    "    cols_to_rename = {\"ENTITY_GROUPS\": \"ENTITY\"}\n",
    "    df_tt = df_ww.copy().drop(['ENTITY'],axis=1).rename(index=str, columns=cols_to_rename)\n",
    "    df_tt['ENTITY_GROUPS'] = 'WORLDWIDE'\n",
    "\n",
    "    df = pd.concat([df,df_con,df_ww,df_tt], axis=0)\n",
    "    df = df.groupby(['ENTITY_GROUPS','ENTITY','DATE'], as_index=False).agg({'CONFIRMED': 'sum','DEATHS': 'sum', 'RECOVERED': 'sum'})\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "db_conso = step3(db_concat,ref_continent)\n",
    "print(\"Script execution completed at \" + datetime.now().strftime('%d/%m/%Y %H:%M:%S') + \". Time: --- %s secnds ---\" % (time.time() - start_time))\n",
    "db_conso.to_csv(output_folder + 'WORLD_DB_CONSO.csv',sep=\";\")\n",
    "# if use_mongo:\n",
    "#     bob.mongo.save_df(db_conso,'WORLD_DB_CONSO',db_src,True)\n",
    "# db_conso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script execution completed at 28/04/2020 06:45:47. Time: --- 4.457710027694702 secnds ---\n"
     ]
    }
   ],
   "source": [
    "#--- STEP4 ---: Calculate KPIs (output = DB_ALL)\n",
    "start_time = time.time()\n",
    "#-> Referentials\n",
    "ref_tech = pd.read_excel(input_folder + 'REF_WSR.xlsx', sheet_name = 'PARAM_ALL')\n",
    "\n",
    "def step4(df,ref):\n",
    "    #-- Calc Active + Ratio\n",
    "    df['ACTIVE_CASES'] = df['CONFIRMED'] - df['DEATHS'] - df['RECOVERED']\n",
    "    df.loc[:,'DEATHS_RATIO'] = 0\n",
    "    df.loc[df['CONFIRMED'] != 0, 'DEATHS_RATIO'] = df['DEATHS'] / df['CONFIRMED'] * 100\n",
    "    df.loc[:,'RECOVERED_RATIO'] = 0\n",
    "    df.loc[df['CONFIRMED'] != 0, 'RECOVERED_RATIO'] = df['RECOVERED'] / df['CONFIRMED'] * 100\n",
    "    \n",
    "    #-- Melt KPI in rows\n",
    "    cols_to_keep = ['ENTITY_GROUPS','ENTITY','DATE']\n",
    "    df= df.melt(id_vars=cols_to_keep,value_name='VALUE',var_name='KPI')\n",
    "    indexes = df.loc[df['DATE'] == 0].index\n",
    "    df = df.drop(indexes, axis=0)\n",
    "    \n",
    "    #-- Variation vs last day\n",
    "    #Add fields\n",
    "    df['DATE'] = pd.to_datetime(df['DATE'], format='%m/%d/%y')\n",
    "    df['LAST_DAY'] =  pd.to_datetime(df['DATE'] + timedelta(days=-1))\n",
    "    \n",
    "    #Create new df\n",
    "    cols_to_rename = {\"DATE\": \"LAST_DAY\",'VALUE':'VALUE_D-1'}\n",
    "    df_last = df.drop(['LAST_DAY'], axis=1).rename(index=str, columns=cols_to_rename)\n",
    "    \n",
    "    #Merge day-1\n",
    "    cols_to_merge = ['ENTITY_GROUPS','ENTITY','LAST_DAY','KPI']\n",
    "    df = df.merge(df_last, on=cols_to_merge, how='left').fillna(0).drop(['LAST_DAY'], axis=1)\n",
    "    \n",
    "    #Calc variation in value and %\n",
    "    df['VARV'] = df['VALUE'].astype(float) - df['VALUE_D-1'].astype(float) \n",
    "    df.loc[:,'VARP'] = np.NaN\n",
    "    df.loc[(df['KPI'].isin(['CONFIRMED','DEATHS','RECOVERED','ACTIVE_CASES'])) & (df['VALUE_D-1'] != 0), 'VARP'] = df['VARV'] * 100 / abs(df['VALUE_D-1'])\n",
    "    \n",
    "    #-- Add fields\n",
    "    df['SCENARIO'] = pd.to_datetime(df['DATE'], format='%m/%d/%y').dt.strftime('%d/%m/%Y')\n",
    "    df['DATE_ORDER'] = pd.to_datetime(df['SCENARIO'], format='%d/%m/%Y').dt.strftime('%Y%m%d')\n",
    "    \n",
    "    #-- Units / Precisions / Sentiments \n",
    "    df['KPI'] = df['KPI'].str.replace('_',' ').str.lower().str.capitalize()\n",
    "    df= pd.merge(df,ref, on=['KPI'], how='left')\n",
    "    df.loc[df['KPI'] == 'Deaths ratio', 'KPI'] = \"Fatality Rate\"\n",
    "    df.loc[df['KPI'] == 'Recovered ratio', 'KPI'] = \"Recovery Rate\"\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "db_all = step4(db_conso,ref_tech)\n",
    "print(\"Script execution completed at \" + datetime.now().strftime('%d/%m/%Y %H:%M:%S') + \". Time: --- %s secnds ---\" % (time.time() - start_time))\n",
    "db_all.to_csv(output_folder + 'WORLD_DB_ALL.csv',sep=\";\")\n",
    "# if use_mongo:\n",
    "#     bob.mongo.save_df(db_all,'WORLD_DB_ALL',db_src,True)\n",
    "# db_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script execution completed at 28/04/2020 06:46:02. Time: --- 10.863661766052246 secnds ---\n"
     ]
    }
   ],
   "source": [
    "#--- STEP5 ---: Transform DB_ALL to trend data (output = DB_TREND)\n",
    "start_time = time.time()\n",
    "#-> Referentials\n",
    "ref_tech = pd.read_excel(input_folder + 'REF_WSR.xlsx', sheet_name = 'PARAM_TREND')\n",
    "\n",
    "def step5(df,ref):\n",
    "    #-- Melt CALC in rows\n",
    "    cols_to_keep = ['ENTITY_GROUPS','ENTITY','SCENARIO','DATE','DATE_ORDER','KPI']\n",
    "    df= df.drop(['VALUE_D-1','UNIT_VALUE','UNIT_VAR','UNIT_VARP','PRECISION_VALUE','PRECISION_VAR','PRECISION_VARP'],axis=1).melt(id_vars=cols_to_keep, value_name='VALUE',var_name='METRIC')\n",
    "    df['DATE_SCENARIO'] = 'Since beginning'\n",
    "        \n",
    "    df_last14 = get_lastdays(df, 14,\"Last 14 days\")\n",
    "    df_last30 = get_lastdays(df, 30,\"Last 30 days\")\n",
    "    df = pd.concat([df,df_last14,df_last30],axis=0)\n",
    "    \n",
    "    #-- Get units and precisions\n",
    "    df= pd.merge(df,ref, on=['KPI','METRIC'], how='left')\n",
    "    df.loc[df['KPI'] == 'Deaths ratio', 'KPI'] = \"Fatality Rate\"\n",
    "    df.loc[df['KPI'] == 'Recovered ratio', 'KPI'] = \"Recovery Rate\"\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "db_trend = step5(db_all,ref_tech)\n",
    "print(\"Script execution completed at \" + datetime.now().strftime('%d/%m/%Y %H:%M:%S') + \". Time: --- %s secnds ---\" % (time.time() - start_time))\n",
    "db_trend.to_csv(output_folder + 'WORLD_DB_TREND.csv',sep=\";\")\n",
    "# if use_mongo:\n",
    "#     bob.mongo.save_df(db_trend,'WORLD_DB_TREND',db_src,True)\n",
    "# db_trend.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data check 2020-04-27: TimeSeries - Daily\n",
      "Confirmed: 3041764 - 3041764 = 0\n",
      "Deaths: 211167 - 211167 = 0\n",
      "Recovered: 893967 - 893967 = 0\n",
      "Active cases: 1936630 - 1936630 = 0\n",
      "Fatality rate: 6.942254560182841 - 6.942254560182841 = 0.0\n",
      "Recovery rate: 29.389755418237574 - 29.389755418237574 = 0.0\n"
     ]
    }
   ],
   "source": [
    "#--- STEP6 ---: Check and validate data\n",
    "#-> CHECK TIMESERIES\n",
    "#change yesterday format to match with header\n",
    "yesterday_c = yesterday.strftime('%m/%d/%y')\n",
    "if yesterday_c[:1]== '0':\n",
    "    yesterday_c = yesterday_c[1:].replace('/0','/')\n",
    "\n",
    "#check if yesterday exist in df\n",
    "def check_timeseries(date,df,df_name):\n",
    "    if date in df.columns:\n",
    "        check = df.agg({date:'sum'}).fillna(0)[0]\n",
    "    else:\n",
    "        check = 0\n",
    "        print(f'Column {date} does not exist in {df_name}.\\n')\n",
    "    return check\n",
    "    \n",
    "check_ct = check_timeseries(yesterday_c,cConfirmed, 'CONFIRMED') + check_timeseries(yesterday_c,cConfirmedUS, 'CONFIRMED')\n",
    "check_dt = check_timeseries(yesterday_c,cDeath, 'DEATHS') + check_timeseries(yesterday_c,cDeathUS, 'DEATHS')\n",
    "check_rt = check_timeseries(yesterday_c,cRecovery, 'RECOVERED')\n",
    "check_at = check_ct - check_dt - check_rt\n",
    "if check_ct != 0:\n",
    "    check_frt = check_dt / check_ct * 100\n",
    "    check_rrt = check_rt / check_ct * 100\n",
    "else:\n",
    "    check_frt = 0\n",
    "    check_rrt = 0\n",
    "\n",
    "#-> CHECK DAILY\n",
    "#check if yesterday exist in df\n",
    "def check_timeseries(date_check,df,variable):\n",
    "    if len(df) != 0 and \"Date\" in df.columns and variable in df.columns and type(date_check) is date:\n",
    "        check = df[df['Date'] == date_check.strftime('%m/%d/%y')].agg({variable:'sum'})[0]\n",
    "    else:\n",
    "        check = 0\n",
    "        print(f'No data in Daily\\n')\n",
    "    return check\n",
    "\n",
    "check_cd = check_timeseries(yesterday,cDaily,'Confirmed')\n",
    "check_dd = check_timeseries(yesterday,cDaily,'Deaths')\n",
    "check_rd = check_timeseries(yesterday,cDaily,'Recovered')\n",
    "check_ad = check_cd - check_dd - check_rd\n",
    "if check_cd != 0:\n",
    "    check_frd = check_dd / check_cd * 100\n",
    "    check_rrd = check_rd / check_cd * 100\n",
    "else:\n",
    "    check_frd = 0\n",
    "    check_rrd = 0\n",
    "    \n",
    "#-- CHECK : TIMESERIES vs DAILY\n",
    "check_c = check_ct - check_cd\n",
    "check_d = check_dt - check_dd\n",
    "check_r = check_rt - check_rd\n",
    "check_a = check_at - check_ad\n",
    "check_fr = check_frt - check_frd\n",
    "check_rr = check_rrt - check_rrd\n",
    "\n",
    "print(f'Data check {yesterday}: TimeSeries - Daily')\n",
    "print(f'Confirmed: {check_ct} - {check_cd} = {check_c}')\n",
    "print(f'Deaths: {check_dt} - {check_dd} = {check_d}')\n",
    "print(f'Recovered: {check_rt} - {check_rd} = {check_r}')\n",
    "print(f'Active cases: {check_at} - {check_ad} = {check_a}')\n",
    "print(f'Fatality rate: {check_frt} - {check_frd} = {check_fr}')\n",
    "print(f'Recovery rate: {check_rrt} - {check_rrd} = {check_rr}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
