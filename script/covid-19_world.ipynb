{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Source COVID World updated everyday at 2:00 AM : </b> <br> \n",
    "https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_time_series <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to MongoDB\n"
     ]
    }
   ],
   "source": [
    "%run __init__.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_to_csv_mongo(url,df_name):\n",
    "    df = check_url(url)\n",
    "    if not df.empty:\n",
    "        # Save in CSV\n",
    "        df_save(df, df_name,'csv')\n",
    "        \n",
    "        # Save in MongoDB\n",
    "        if USE_MONGO:\n",
    "            naas_drivers.mongo.send(df, df_name, DB_SRC,True)\n",
    "    else:\n",
    "        print(f'DataFrame {df_name} is empty !')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv'\n",
    "# df = pd.read_csv(url, sep=',')\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORLD_CONFIRMED successfully saved in WORLD_CONFIRMED\n",
      "Dataframe WORLD_CONFIRMED successfully save in database covid-19 in MongoDB. Time: --- 0.15373945236206055 secnds ---\n",
      "WORLD_DEATHS successfully saved in WORLD_DEATHS\n",
      "Dataframe WORLD_DEATHS successfully save in database covid-19 in MongoDB. Time: --- 0.1858377456665039 secnds ---\n",
      "WORLD_RECOVERED successfully saved in WORLD_RECOVERED\n",
      "Dataframe WORLD_RECOVERED successfully save in database covid-19 in MongoDB. Time: --- 0.14579057693481445 secnds ---\n",
      "US_CONFIRMED successfully saved in US_CONFIRMED\n",
      "Dataframe US_CONFIRMED successfully save in database covid-19 in MongoDB. Time: --- 6.322901248931885 secnds ---\n",
      "US_DEATHS successfully saved in US_DEATHS\n",
      "Dataframe US_DEATHS successfully save in database covid-19 in MongoDB. Time: --- 6.366649150848389 secnds ---\n",
      "CPU times: user 5.47 s, sys: 276 ms, total: 5.74 s\n",
      "Wall time: 14.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#--- STEP1 ---: Get data source & cleaning\n",
    "\n",
    "# RACINE_URL = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/'\n",
    "\n",
    "#-- GITHUB : TIMESERIES (CSSEGISandData/COVID-19/csse_covid_19_time_series)\n",
    "#-> WORLD_CONFIRMED\n",
    "urlConfirmed = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv'\n",
    "cConfirmed = url_to_csv_mongo(urlConfirmed,'WORLD_CONFIRMED')\n",
    "\n",
    "#-> WORLD_DEATHS\n",
    "urlDeath = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv'\n",
    "cDeath = url_to_csv_mongo(urlDeath,'WORLD_DEATHS')\n",
    "\n",
    "#-> WORLD_RECOVERED\n",
    "urlRecovery = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_recovered_global.csv'\n",
    "cRecovery = url_to_csv_mongo(urlRecovery,'WORLD_RECOVERED')\n",
    "\n",
    "#-> US_CONFIRMED\n",
    "urlConfirmedUS = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv'\n",
    "cConfirmedUS = url_to_csv_mongo(urlConfirmedUS,'US_CONFIRMED')\n",
    "\n",
    "#-> US_DEATHS\n",
    "urlDeathUS = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_US.csv'\n",
    "cDeathUS = url_to_csv_mongo(urlDeathUS,'US_DEATHS')\n",
    "\n",
    "#-- GITHUB : DAILY REPORT (CSSEGISandData/COVID-19/csse_covid_19_time_series)\n",
    "#-> DAILY\n",
    "date_init = YESTERDAY.strftime('%m-%d-%Y')\n",
    "date_end = YESTERDAY.strftime('%m-%d-%Y')\n",
    "dates = pd.date_range(start=date_init,end=date_end, freq='D').strftime('%m-%d-%Y').values.tolist()\n",
    "filter_dates = pd.date_range(start=date_init,end=date_end, freq='D').strftime('%m/%d/%y').values.tolist()\n",
    "\n",
    "cDaily = pd.DataFrame()\n",
    "for d in dates:\n",
    "    'https://github.com/CSSEGISandData/COVID-19/blob/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv'\n",
    "    urlDaily = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports/' + d + '.csv'\n",
    "    cols_to_rename = {\"Country/Region\": \"Country_Region\",\"Province/State\": \"Province_State\", \"Long_\":\"Long\"}\n",
    "    tmp_df = check_url(urlDaily).rename(index=str, columns=cols_to_rename)\n",
    "    if len(tmp_df) != 0:\n",
    "        tmp_df['Confirmed'] = tmp_df['Confirmed'].fillna(0).astype(int)\n",
    "        tmp_df['Deaths'] = tmp_df['Deaths'].fillna(0).astype(int)\n",
    "        tmp_df['Recovered'] = tmp_df['Recovered'].fillna(0).astype(int)\n",
    "        tmp_df['Date'] = pd.to_datetime(d, format='%m-%d-%Y').strftime('%m/%d/%y')\n",
    "        cDaily = cDaily.append(tmp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.08 s, sys: 112 ms, total: 2.19 s\n",
      "Wall time: 2.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#-- Melt data source from timeseries to have date in rows\n",
    "def melt_cleaning(df,variable):\n",
    "    try:\n",
    "        if 'Province/State' and 'Country/Region' in df.columns:\n",
    "            #Add Country/Region to Province/State if empty\n",
    "            df = df.fillna(0)\n",
    "            df.loc[df['Province/State'] == 0,'Province/State'] = df['Country/Region']\n",
    "            #Melt and group data\n",
    "            cols_to_keep = ['Country/Region','Province/State']\n",
    "            cols_to_group = ['Country/Region','Province/State','Date']\n",
    "            df = pd.melt(df, id_vars=cols_to_keep,value_name=variable,var_name='Date')\n",
    "            df = df.groupby(cols_to_group, as_index=False).agg({variable:'sum'})\n",
    "            #Data format\n",
    "            df['Date'] = pd.to_datetime(df['Date'], format='%m/%d/%y').dt.strftime('%m/%d/%y')\n",
    "\n",
    "            if variable == 'Confirmed':\n",
    "                df['Deaths'] = 0\n",
    "                df['Recovered'] = 0\n",
    "            elif variable == 'Deaths':\n",
    "                df['Confirmed'] = 0\n",
    "                df['Recovered'] = 0\n",
    "            else:\n",
    "                df['Confirmed'] = 0\n",
    "                df['Deaths'] = 0\n",
    "        else:\n",
    "            df=pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        df=pd.DataFrame()\n",
    "        print(e.__doc__)\n",
    "        print(str(e))  \n",
    "    return df\n",
    "\n",
    "def cols_cleaning(df,cols_to_drop):\n",
    "    for c in cols_to_drop:\n",
    "        if c in df.columns:\n",
    "            df = df.drop(c,axis=1)\n",
    "            \n",
    "    cols_to_rename = {\"Province_State\": \"Province/State\", \"Country_Region\": \"Country/Region\"}\n",
    "    df = df.rename(index=str, columns=cols_to_rename)\n",
    "    return df       \n",
    "\n",
    "# Timeseries : World Confirmed\n",
    "cols_to_drop = ['Lat','Long']\n",
    "cConfirmed=cConfirmed[cConfirmed['Country/Region'] != 'US']\n",
    "df_cW = melt_cleaning(cols_cleaning(cConfirmed, cols_to_drop),'Confirmed')\n",
    "    \n",
    "# Timeseries : World Deaths\n",
    "cols_to_drop = ['Lat','Long']\n",
    "cDeath=cDeath[cDeath['Country/Region'] != 'US']\n",
    "df_dW = melt_cleaning(cols_cleaning(cDeath, cols_to_drop),'Deaths')\n",
    "    \n",
    "# Timeseries : World Recovered\n",
    "cols_to_drop = ['Lat','Long']\n",
    "df_rW = melt_cleaning(cols_cleaning(cRecovery, cols_to_drop),'Recovered')\n",
    "\n",
    "# Timeseries : US Confirmed\n",
    "cols_to_drop = ['UID','iso2', 'iso3','code3','FIPS','Admin2','Lat','Long_','Combined_Key']\n",
    "df_cUS = melt_cleaning(cols_cleaning(cConfirmedUS, cols_to_drop),'Confirmed')\n",
    "\n",
    "# Timeseries : US Deaths\n",
    "cols_to_drop = ['UID','iso2', 'iso3','code3','FIPS','Admin2','Lat','Long_','Combined_Key','Population']\n",
    "df_dUS = melt_cleaning(cols_cleaning(cDeathUS, cols_to_drop),'Deaths')\n",
    "\n",
    "# Manual input\n",
    "df_manual = pd.read_excel(REF_WSR_PATH, sheet_name = 'DATA_INPUT').fillna(0)\n",
    "df_manual['Date'] = pd.to_datetime(df_manual['Date'], format='%Y%m%d').dt.strftime('%m/%d/%y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORLD_DB_CONCAT successfully saved in WORLD_DB_CONCAT\n",
      "CPU times: user 486 ms, sys: 23.4 ms, total: 509 ms\n",
      "Wall time: 625 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#--- STEP2 ---: Create DB with data source (output = DB_CONCAT)\n",
    "def step2(df1,df2,df3,df4,df5,df6):\n",
    "    #-- Merge timeseries\n",
    "    cols_to_group = ['Country/Region','Province/State','Date']\n",
    "    df = pd.concat([df1,df2,df3,df4,df5,df6], axis=0).groupby(cols_to_group, as_index=False).agg({'Confirmed':'sum','Deaths':'sum','Recovered':'sum'})\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "db_concat = step2(df_cW,df_dW,df_rW,df_cUS,df_dUS,df_manual)\n",
    "df_save(db_concat, 'WORLD_DB_CONCAT','csv')\n",
    "# db_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data updated, 2020-11-03 exits in WORLD_DB_CONCAT\n",
      "CPU times: user 10 ms, sys: 137 µs, total: 10.2 ms\n",
      "Wall time: 10.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#--- Check if date is updated\n",
    "df_check = db_concat[db_concat['Date'] == YESTERDAY.strftime('%m/%d/%y')]\n",
    "continue_script = True\n",
    "if len(df_check) == 0:\n",
    "    print(f'Data not updated !')\n",
    "    continue_script = False\n",
    "else:\n",
    "    print(f\"Data updated, {YESTERDAY} exits in WORLD_DB_CONCAT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORLD_DB_CONSO successfully saved in WORLD_DB_CONSO\n",
      "CPU times: user 1.13 s, sys: 59.3 ms, total: 1.19 s\n",
      "Wall time: 1.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#--- STEP3 ---: Consolidate and enrich data (output = DB_CONSO)\n",
    "#-> Referentials\n",
    "ref_continent = pd.read_excel(REF_WSR_PATH, sheet_name = 'REF_CONTINENT')\n",
    "def step3(df,ref):\n",
    "    df.columns = df.columns.str.upper()\n",
    "    cols_to_rename = {\"COUNTRY/REGION\": \"ENTITY_GROUPS\", \"PROVINCE/STATE\": \"ENTITY\"}\n",
    "    df = df.rename(index=str, columns=cols_to_rename)\n",
    "    \n",
    "    cols_to_rename = {\"ENTITY_GROUPS\": \"ENTITY\", \"CONTINENT\": \"ENTITY_GROUPS\"}\n",
    "    df_con = pd.merge(df.drop(['ENTITY'],axis=1),ref.drop(['WORLDMAP'],axis=1), left_on=['ENTITY_GROUPS'], right_on=['COUNTRY_REGION'], how='left').rename(index=str, columns=cols_to_rename).drop(['COUNTRY_REGION'],axis=1).fillna(0)\n",
    "    df_con.loc[df_con['ENTITY_GROUPS'] == 0, 'ENTITY_GROUPS'] = 'To be affected'\n",
    "    \n",
    "    cols_to_rename = {\"ENTITY_GROUPS\": \"ENTITY\"}\n",
    "    df_ww = df_con.copy().drop(['ENTITY'],axis=1).rename(index=str, columns=cols_to_rename)\n",
    "    df_ww['ENTITY_GROUPS'] = 'WORLDWIDE'\n",
    "    \n",
    "    cols_to_rename = {\"ENTITY_GROUPS\": \"ENTITY\"}\n",
    "    df_tt = df_ww.copy().drop(['ENTITY'],axis=1).rename(index=str, columns=cols_to_rename)\n",
    "    df_tt['ENTITY_GROUPS'] = 'WORLDWIDE'\n",
    "\n",
    "    df = pd.concat([df,df_con,df_ww,df_tt], axis=0)\n",
    "    df = df.groupby(['ENTITY_GROUPS','ENTITY','DATE'], as_index=False).agg({'CONFIRMED': 'sum','DEATHS': 'sum', 'RECOVERED': 'sum'})\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "if continue_script:\n",
    "    db_conso = step3(db_concat,ref_continent)\n",
    "    df_save(db_conso, 'WORLD_DB_CONSO','csv')\n",
    "# db_conso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORLD_DB_ALL successfully saved in WORLD_DB_ALL\n",
      "CPU times: user 22.6 s, sys: 614 ms, total: 23.3 s\n",
      "Wall time: 25.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#--- STEP4 ---: Calculate KPIs (output = DB_ALL)\n",
    "#-> Referentials\n",
    "ref_tech = pd.read_excel(REF_WSR_PATH, sheet_name = 'PARAM_ALL')\n",
    "\n",
    "def step4(df,ref):\n",
    "    #-- Calc Active + Ratio\n",
    "    df['ACTIVE_CASES'] = df['CONFIRMED'] - df['DEATHS'] - df['RECOVERED']\n",
    "    df.loc[:,'DEATHS_RATIO'] = 0\n",
    "    df.loc[df['CONFIRMED'] != 0, 'DEATHS_RATIO'] = df['DEATHS'] / df['CONFIRMED'] * 100\n",
    "    df.loc[:,'RECOVERED_RATIO'] = 0\n",
    "    df.loc[df['CONFIRMED'] != 0, 'RECOVERED_RATIO'] = df['RECOVERED'] / df['CONFIRMED'] * 100\n",
    "    \n",
    "    #-- Melt KPI in rows\n",
    "    cols_to_keep = ['ENTITY_GROUPS','ENTITY','DATE']\n",
    "    df= df.melt(id_vars=cols_to_keep,value_name='VALUE',var_name='KPI')\n",
    "    indexes = df.loc[df['DATE'] == 0].index\n",
    "    df = df.drop(indexes, axis=0)\n",
    "    \n",
    "    #-- Variation vs last day\n",
    "    #Add fields\n",
    "    df['DATE'] = pd.to_datetime(df['DATE'], format='%m/%d/%y')\n",
    "    df['LAST_DAY'] =  pd.to_datetime(df['DATE'] + timedelta(days=-1))\n",
    "    \n",
    "    #Create new df\n",
    "    cols_to_rename = {\"DATE\": \"LAST_DAY\",'VALUE':'VALUE_D-1'}\n",
    "    df_last = df.drop(['LAST_DAY'], axis=1).rename(index=str, columns=cols_to_rename)\n",
    "    \n",
    "    #Merge day-1\n",
    "    cols_to_merge = ['ENTITY_GROUPS','ENTITY','LAST_DAY','KPI']\n",
    "    df = df.merge(df_last, on=cols_to_merge, how='left').fillna(0).drop(['LAST_DAY'], axis=1)\n",
    "    \n",
    "    #Calc variation in value and %\n",
    "    df['VARV'] = df['VALUE'].astype(float) - df['VALUE_D-1'].astype(float) \n",
    "    df.loc[:,'VARP'] = np.NaN\n",
    "    df.loc[(df['KPI'].isin(['CONFIRMED','DEATHS','RECOVERED','ACTIVE_CASES'])) & (df['VALUE_D-1'] != 0), 'VARP'] = df['VARV'] * 100 / abs(df['VALUE_D-1'])\n",
    "    \n",
    "    #-- Add fields\n",
    "    df['SCENARIO'] = pd.to_datetime(df['DATE'], format='%m/%d/%y').dt.strftime('%d/%m/%Y')\n",
    "    df['DATE_ORDER'] = pd.to_datetime(df['SCENARIO'], format='%d/%m/%Y').dt.strftime('%Y%m%d')\n",
    "    \n",
    "    #-- Units / Precisions / Sentiments \n",
    "    df['KPI'] = df['KPI'].str.replace('_',' ').str.lower().str.capitalize()\n",
    "    df= pd.merge(df,ref, on=['KPI'], how='left')\n",
    "    df.loc[df['KPI'] == 'Deaths ratio', 'KPI'] = \"Fatality Rate\"\n",
    "    df.loc[df['KPI'] == 'Recovered ratio', 'KPI'] = \"Recovery Rate\"\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "if continue_script:\n",
    "    db_all = step4(db_conso,ref_tech)\n",
    "    db_all = optimize(db_all, ['DATE'])\n",
    "    df_save(db_all, 'WORLD_DB_ALL','csv')\n",
    "# db_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORLD_DB_TREND successfully saved in WORLD_DB_TREND\n",
      "CPU times: user 51.6 s, sys: 1.04 s, total: 52.7 s\n",
      "Wall time: 1min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#--- STEP5 ---: Transform DB_ALL to trend data (output = DB_TREND)\n",
    "#-> Referentials\n",
    "ref_tech = pd.read_excel(REF_WSR_PATH, sheet_name = 'PARAM_TREND')\n",
    "\n",
    "def step5(df,ref):\n",
    "    #-- Melt CALC in rows\n",
    "    cols_to_keep = ['ENTITY_GROUPS','ENTITY','SCENARIO','DATE','DATE_ORDER','KPI']\n",
    "    df = df.drop(['VALUE_D-1','UNIT_VALUE','UNIT_VAR','UNIT_VARP','PRECISION_VALUE','PRECISION_VAR','PRECISION_VARP'],axis=1)\n",
    "    df = pd.melt(df, id_vars=cols_to_keep, value_name='VALUE',var_name='METRIC')\n",
    "    df['DATE_SCENARIO'] = 'Since beginning'\n",
    "        \n",
    "    df_last14 = get_lastdays(df, 14,\"Last 14 days\")\n",
    "    df_last30 = get_lastdays(df, 30,\"Last 30 days\")\n",
    "    df = pd.concat([df,df_last14,df_last30],axis=0)\n",
    "    \n",
    "    #-- Get units and precisions\n",
    "    df= pd.merge(df,ref, on=['KPI','METRIC'], how='left')\n",
    "    df.loc[df['KPI'] == 'Deaths ratio', 'KPI'] = \"Fatality Rate\"\n",
    "    df.loc[df['KPI'] == 'Recovered ratio', 'KPI'] = \"Recovery Rate\"\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "if continue_script:\n",
    "    db_trend = step5(db_all,ref_tech)\n",
    "    db_trend = optimize(db_trend)\n",
    "    df_save(db_trend, 'WORLD_DB_TREND','csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 11/03/20 does not exist in CONFIRMED.\n",
      "\n",
      "Column 11/03/20 does not exist in CONFIRMED.\n",
      "\n",
      "Column 11/03/20 does not exist in DEATHS.\n",
      "\n",
      "Column 11/03/20 does not exist in DEATHS.\n",
      "\n",
      "Column 11/03/20 does not exist in RECOVERED.\n",
      "\n",
      "Data check 2020-11-03: TimeSeries - Daily\n",
      "Confirmed: 0 - 47405395 = -47405395\n",
      "Deaths: 0 - 1213735 = -1213735\n",
      "Recovered: 0 - 31609242 = -31609242\n",
      "Active cases: 0 - 14582418 = -14582418\n",
      "Fatality rate: 0 - 2.560330949673555 = -2.560330949673555\n",
      "Recovery rate: 0 - 66.67857529717874 = -66.67857529717874\n"
     ]
    }
   ],
   "source": [
    "#--- STEP6 ---: Check and validate data\n",
    "#-> CHECK TIMESERIES\n",
    "#change yesterday format to match with header\n",
    "\n",
    "def check_and_validate():\n",
    "    YESTERDAY_C = YESTERDAY.strftime('%m/%d/%y')\n",
    "    if YESTERDAY_C[:1]== '0':\n",
    "        YESTERDAY_C = YESTERDAY_C[1:].replace('/0','/')\n",
    "\n",
    "    #check if yesterday exist in df\n",
    "    def check_timeseries(date,df,df_name):\n",
    "        if date in df.columns:\n",
    "            check = df.agg({date:'sum'}).fillna(0)[0]\n",
    "        else:\n",
    "            check = 0\n",
    "            print(f'Column {date} does not exist in {df_name}.\\n')\n",
    "        return check\n",
    "\n",
    "    check_ct = check_timeseries(YESTERDAY_C,cConfirmed, 'CONFIRMED') + check_timeseries(YESTERDAY_C,cConfirmedUS, 'CONFIRMED')\n",
    "    check_dt = check_timeseries(YESTERDAY_C,cDeath, 'DEATHS') + check_timeseries(YESTERDAY_C,cDeathUS, 'DEATHS')\n",
    "    check_rt = check_timeseries(YESTERDAY_C,cRecovery, 'RECOVERED')\n",
    "    check_at = check_ct - check_dt - check_rt\n",
    "    if check_ct != 0:\n",
    "        check_frt = check_dt / check_ct * 100\n",
    "        check_rrt = check_rt / check_ct * 100\n",
    "    else:\n",
    "        check_frt = 0\n",
    "        check_rrt = 0\n",
    "\n",
    "    #-> CHECK DAILY\n",
    "    #check if yesterday exist in df\n",
    "    def check_timeseries(date_check,df,variable):\n",
    "        if len(df) != 0 and \"Date\" in df.columns and variable in df.columns and type(date_check) is date:\n",
    "            check = df[df['Date'] == date_check.strftime('%m/%d/%y')].agg({variable:'sum'})[0]\n",
    "        else:\n",
    "            check = 0\n",
    "            print(f'No data in Daily\\n')\n",
    "        return check\n",
    "\n",
    "    check_cd = check_timeseries(YESTERDAY,cDaily,'Confirmed')\n",
    "    check_dd = check_timeseries(YESTERDAY,cDaily,'Deaths')\n",
    "    check_rd = check_timeseries(YESTERDAY,cDaily,'Recovered')\n",
    "    check_ad = check_cd - check_dd - check_rd\n",
    "    if check_cd != 0:\n",
    "        check_frd = check_dd / check_cd * 100\n",
    "        check_rrd = check_rd / check_cd * 100\n",
    "    else:\n",
    "        check_frd = 0\n",
    "        check_rrd = 0\n",
    "\n",
    "    #-- CHECK : TIMESERIES vs DAILY\n",
    "    check_c = check_ct - check_cd\n",
    "    check_d = check_dt - check_dd\n",
    "    check_r = check_rt - check_rd\n",
    "    check_a = check_at - check_ad\n",
    "    check_fr = check_frt - check_frd\n",
    "    check_rr = check_rrt - check_rrd\n",
    "\n",
    "    print(f'Data check {YESTERDAY}: TimeSeries - Daily')\n",
    "    print(f'Confirmed: {check_ct} - {check_cd} = {check_c}')\n",
    "    print(f'Deaths: {check_dt} - {check_dd} = {check_d}')\n",
    "    print(f'Recovered: {check_rt} - {check_rd} = {check_r}')\n",
    "    print(f'Active cases: {check_at} - {check_ad} = {check_a}')\n",
    "    print(f'Fatality rate: {check_frt} - {check_frd} = {check_fr}')\n",
    "    print(f'Recovery rate: {check_rrt} - {check_rrd} = {check_rr}')\n",
    "    \n",
    "if continue_script:\n",
    "    check_and_validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
